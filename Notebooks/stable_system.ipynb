{"cells":[{"cell_type":"markdown","metadata":{"id":"rq2c9LClTEPh"},"source":["# Notebook Initialization"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_j14DIQ62Ph0","executionInfo":{"status":"ok","timestamp":1737755426359,"user_tz":-60,"elapsed":27635,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}},"outputId":"25317153-2d3c-4d50-e632-4c49e02ddc0d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (3.3.1)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.47.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.67.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.5.1+cu121)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.13.1)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.27.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (11.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.17.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.9.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.5)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.6.85)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.12.14)\n","Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.9.0.post1)\n","Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n","\u001b[0mRequirement already satisfied: scann in /usr/local/lib/python3.11/dist-packages (1.3.5)\n","Requirement already satisfied: tensorflow~=2.18.0 in /usr/local/lib/python3.11/dist-packages (from scann) (2.18.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from scann) (1.26.4)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->scann) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->scann) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->scann) (25.1.21)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->scann) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->scann) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->scann) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->scann) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->scann) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->scann) (4.25.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->scann) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->scann) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->scann) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->scann) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->scann) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->scann) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->scann) (1.69.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->scann) (2.18.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->scann) (3.5.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->scann) (3.12.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->scann) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->scann) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow~=2.18.0->scann) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow~=2.18.0->scann) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow~=2.18.0->scann) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow~=2.18.0->scann) (0.14.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow~=2.18.0->scann) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow~=2.18.0->scann) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow~=2.18.0->scann) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow~=2.18.0->scann) (2024.12.14)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow~=2.18.0->scann) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow~=2.18.0->scann) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow~=2.18.0->scann) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow~=2.18.0->scann) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow~=2.18.0->scann) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow~=2.18.0->scann) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow~=2.18.0->scann) (0.1.2)\n","Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (3.11.0)\n","Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.11/dist-packages (0.26.1)\n","Requirement already satisfied: Levenshtein==0.26.1 in /usr/local/lib/python3.11/dist-packages (from python-Levenshtein) (0.26.1)\n","Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from Levenshtein==0.26.1->python-Levenshtein) (3.11.0)\n","Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank-bm25) (1.26.4)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n","Collecting en-core-web-sm==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}],"source":["!pip install datasets\n","!pip install sentence_transformers\n","!pip install faiss-cpu\n","!pip install faiss-gpu\n","!pip install scann\n","!pip install rapidfuzz\n","!pip install python-Levenshtein\n","!pip install rank-bm25\n","!pip install spacy\n","!python -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"0Rq-rgwZlMXT","executionInfo":{"status":"ok","timestamp":1737755426360,"user_tz":-60,"elapsed":15,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}}},"outputs":[],"source":["from google.colab import drive\n","from typing import Any, Callable, Iterable\n","from sentence_transformers import SentenceTransformer\n","from tqdm import tqdm\n","from rapidfuzz.process import cdist\n","from rank_bm25 import BM25Okapi\n","import os\n","import time\n","import re\n","import json\n","import copy\n","import pickle\n","import enum\n","import torch\n","import faiss\n","import scann\n","import spacy\n","import Levenshtein\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["drive.mount('/content/drive')\n","\n","DATASET_ROOT = '/content/drive/MyDrive/ADSP Project/datasets/'\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","RETRIEVAL_CAPACITY = 100\n","\n","if not os.path.exists(DATASET_ROOT):\n","    raise ValueError('Invalid data root')"],"metadata":{"id":"mhD5hshbLqsP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737755427877,"user_tz":-60,"elapsed":1529,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}},"outputId":"e1e26c04-abd5-4861-eeb6-aa15f6acac82"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"XFd-W_zwTIUV"},"source":["# Classes"]},{"cell_type":"markdown","source":["## Config"],"metadata":{"id":"dUZQ4D2K79Bb"}},{"cell_type":"code","source":["class Config:\n","\n","    class DATASET_NAMES(enum.Enum):\n","        MS_MARCO = 'ms-marco'\n","        HOTPOT_QA = 'hotpot-qa'\n","\n","    class TRANSFORMER_MODEL_NAMES(enum.Enum):\n","        ALL_MPNET_BASE_V2 = 'all-mpnet-base-v2'\n","        MULTI_QA_MPNET_BASE_DOT_V1 = 'multi-qa-mpnet-base-dot-v1'\n","        ALL_DISTILROBERTA_V1 = 'all-distilroberta-v1'\n","\n","    class VECTOR_DB_NAMES(enum.Enum):\n","        FAISS = 'faiss'\n","        SCANN = 'scann'\n","\n","    class SIMILARITY_METRIC_NAMES(enum.Enum):\n","        L2 = 'l2'\n","        IP = 'ip'\n","        CS = 'cs'\n","\n","    class SCORER_NAMES(enum.Enum):\n","        SIMPLE = 'simple'\n","        LEVENSHTEIN = 'levenshtein'\n","        JARO_WINKLER = 'jaro_winkler'\n","\n","    class AGGREGATOR_NAMES(enum.Enum):\n","        MIN_MIN_0 = 'min-min-0'\n","        MIN_AVG_0 = 'min-avg-0'\n","        MIN_MAX_0 = 'min-max-0'\n","        AVG_MIN_0 = 'avg-min-0'\n","        AVG_AVG_0 = 'avg-avg-0'\n","        AVG_MAX_0 = 'avg-max-0'\n","        MAX_MIN_0 = 'max-min-0'\n","        MAX_AVG_0 = 'max-avg-0'\n","        MAX_MAX_0 = 'max-max-0'\n","        MIN_MIN_1 = 'min-min-1'\n","        MIN_AVG_1 = 'min-avg-1'\n","        MIN_MAX_1 = 'min-max-1'\n","        AVG_MIN_1 = 'avg-min-1'\n","        AVG_AVG_1 = 'avg-avg-1'\n","        AVG_MAX_1 = 'avg-max-1'\n","        MAX_MIN_1 = 'max-min-1'\n","        MAX_AVG_1 = 'max-avg-1'\n","        MAX_MAX_1 = 'max-max-1'\n","\n","    class TOKENIZER_NAMES(enum.Enum):\n","        SIMPLE = 'simple'\n","        LEMMA = 'lemmatization'"],"metadata":{"id":"G3LoTJBl7_kd","executionInfo":{"status":"ok","timestamp":1737755472358,"user_tz":-60,"elapsed":484,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["## Dataset"],"metadata":{"id":"sHyt5bXp8AYv"}},{"cell_type":"code","source":["class Dataset:\n","\n","    def __init__(self, file_name:str) -> None:\n","        self._file_name:str = file_name\n","        self._stat_dict = {\n","            'passages': dict[str, int](),\n","            'queries': dict[str, int](),\n","            'augmentations': dict[str, int](),\n","            'relations': dict[str, int](),\n","            'learning': dict[str, int]()\n","        }\n","        self.dataset_name:Config.DATASET_NAMES = None\n","        self.passage_list = list[str]()\n","        self.query_list = list[str]()\n","        self.passage_augmentation_list = list[dict[str, dict[str, int]]]()\n","        self.query_augmentation_list = list[dict[str, dict[str, int]]]()\n","        self.augmentation_dict = dict[str, set[int]]()\n","        self.relation_list = list[set[int]]()\n","        self.train_set = set[int]()\n","        self.validation_set = set[int]()\n","        self.test_set = set[int]()\n","        potential_dataset_path = os.path.join(DATASET_ROOT, f'{file_name}.pickle')\n","        if os.path.exists(potential_dataset_path):\n","            with open(potential_dataset_path, 'rb') as file_handle:\n","                public_dataset = pickle.load(file_handle)\n","                for attribute in public_dataset:\n","                    setattr(self, attribute, public_dataset[attribute])\n","            if self.dataset_name not in {item.value for item in Config.DATASET_NAMES}:\n","                raise ValueError('Invalid dataset name')\n","            self.dataset_name = Config.DATASET_NAMES(self.dataset_name)\n","        else:\n","            raise ValueError('Invalid file name')\n","        self._update_stat()\n","\n","    def __str__(self) -> str:\n","        output_list = [f'names -> file: {self._file_name}, dataset: {self.dataset_name}']\n","        for stat in self._stat_dict:\n","            if len(self._stat_dict[stat]) == 0:\n","                continue\n","            output_list.append(f'{stat} -> ' + ', '.join(f'{attribute}: {self._stat_dict[stat][attribute]}' for attribute in self._stat_dict[stat]))\n","        return '\\n'.join(output_list)\n","\n","    def _update_stat(self) -> None:\n","        def __count_quantity(key:str, suffix:str, target_list:list[Any]) -> None:\n","            self._stat_dict[key][f'total_{suffix}'] = len(target_list)\n","        def __compute_stat(key:str, suffix:str, target_list:list[Iterable]) -> None:\n","            if len(target_list) > 0:\n","                self._stat_dict[key][f'minimum_{suffix}'] = min(len(iterable) for iterable in target_list)\n","                self._stat_dict[key][f'average_{suffix}'] = round(sum(len(iterable) for iterable in target_list) / len(target_list))\n","                self._stat_dict[key][f'maximum_{suffix}'] = max(len(iterable) for iterable in target_list)\n","        __count_quantity('passages', '', self.passage_list)\n","        __compute_stat('passages', 'length', self.passage_list)\n","        __count_quantity('queries', '', self.query_list)\n","        __compute_stat('queries', 'length', self.query_list)\n","        for augmentation_name in self.augmentation_dict:\n","            __count_quantity('augmentations', f'queries_augmented_with_{augmentation_name}', self.augmentation_dict[augmentation_name])\n","        __compute_stat('relations', 'related_passages', self.relation_list)\n","        __count_quantity('learning', 'queries_in_train_set', self.train_set)\n","        __count_quantity('learning', 'queries_in_validation_set', self.validation_set)\n","        __count_quantity('learning', 'queries_in_test_set', self.test_set)\n","\n","    def _get_recall(self, query_index:int, query_retrieved_passage_indices:np.ndarray) -> list[float]:\n","        total_related_passages = len(self.relation_list[query_index])\n","        recall_list = list[float]()\n","        for k in range(1, query_retrieved_passage_indices.size + 1):\n","            recall_list.append(len(self.relation_list[query_index].intersection(query_retrieved_passage_indices[:k])) / total_related_passages)\n","        return recall_list\n","\n","    def _get_optimistic_mrr(self, query_index:int, query_retrieved_passage_indices:np.ndarray) -> float:\n","        optimistic_mrr = 0.0\n","        for rank, retrieved_passage_index in enumerate(query_retrieved_passage_indices, start=1):\n","            if retrieved_passage_index in self.relation_list[query_index]:\n","                optimistic_mrr = 1.0 / rank\n","                break\n","        return optimistic_mrr\n","\n","    def _get_pessimistic_mrr(self, query_index:int, query_retrieved_passage_indices:np.ndarray) -> float:\n","        total_related_passages = len(self.relation_list[query_index])\n","        pessimistic_mrr = 0.0\n","        for rank in range(total_related_passages, query_retrieved_passage_indices.size + 1):\n","            if len(self.relation_list[query_index].intersection(query_retrieved_passage_indices[:rank])) == total_related_passages:\n","                pessimistic_mrr = total_related_passages / rank\n","                break\n","        return pessimistic_mrr\n","\n","    def get_metrics(self, query_index_list:list[int], retrieved_passage_indices:np.ndarray) -> tuple[dict[int, float], dict[int, float], float, float]:\n","        recall_dict = dict[int, list[float]]()\n","        recall_star_dict = dict[int, list[float]]()\n","        optimistic_mrr_list = list[float]()\n","        pessimistic_mrr_list = list[float]()\n","        for i in range(len(query_index_list)):\n","            total_related_passages = len(self.relation_list[query_index_list[i]])\n","            if total_related_passages == 0:\n","                continue\n","            recall_list = self._get_recall(query_index_list[i], retrieved_passage_indices[i, :])\n","            optimistic_mrr = self._get_optimistic_mrr(query_index_list[i], retrieved_passage_indices[i, :])\n","            pessimistic_mrr = self._get_pessimistic_mrr(query_index_list[i], retrieved_passage_indices[i, :])\n","            for k, recall in enumerate(recall_list, start=1):\n","                if k not in recall_dict:\n","                    recall_dict[k] = list[float]()\n","                recall_dict[k].append(recall)\n","                if k == total_related_passages:\n","                    if total_related_passages not in recall_star_dict:\n","                        recall_star_dict[total_related_passages] = list[float]()\n","                    recall_star_dict[total_related_passages].append(recall)\n","            optimistic_mrr_list.append(optimistic_mrr)\n","            pessimistic_mrr_list.append(pessimistic_mrr)\n","        avg_recall_dict = {k: sum(recall_list) / len(recall_list) for k, recall_list in dict(sorted(recall_dict.items())).items()}\n","        avg_recall_start_dict = {total_related_passages: sum(recall_star_list) / len(recall_star_list) for total_related_passages, recall_star_list in dict(sorted(recall_star_dict.items())).items()}\n","        avg_optimistic_mrr = sum(optimistic_mrr_list) / len(optimistic_mrr_list)\n","        avg_pessimistic_mrr = sum(pessimistic_mrr_list) / len(pessimistic_mrr_list)\n","        return avg_recall_dict, avg_recall_start_dict, avg_optimistic_mrr, avg_pessimistic_mrr\n","\n","    def print_metrics(self, query_index_list:list[int], retrieved_passage_indices:np.ndarray) -> None:\n","        avg_recall_dict, avg_recall_start_dict, avg_optimistic_mrr, avg_pessimistic_mrr = self.get_metrics(query_index_list, retrieved_passage_indices)\n","        print('Dataset Name ->', self.dataset_name)\n","        print('Recall ->', ' | '.join(f'{k}: {100 * avg_recall:.2f}%' for k, avg_recall in avg_recall_dict.items()))\n","        print('Cluster Recall ->', ' | '.join(f'{total_related_passages}: {100 * avg_recall:.2f}%' for total_related_passages, avg_recall in avg_recall_start_dict.items()))\n","        print('MRR ->', f'optimistic: {100 * avg_optimistic_mrr:.2f}% | pessimistic: {100 * avg_pessimistic_mrr:.2f}%')"],"metadata":{"id":"_8OO2NbUsGR4","executionInfo":{"status":"ok","timestamp":1737755472706,"user_tz":-60,"elapsed":353,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## Transformer"],"metadata":{"id":"B9ja7XCi8HUx"}},{"cell_type":"code","source":["class Transformer:\n","\n","    def __init__(self, model_name:Config.TRANSFORMER_MODEL_NAMES) -> None:\n","        self.model_name = model_name\n","        self._transformer = SentenceTransformer(model_name_or_path=model_name.value, device=DEVICE)\n","        self._transformer.encode(['warm_up'])\n","\n","    def embed(self, text_list:list[str]) -> np.ndarray:\n","        print('Embedding ...', end='')\n","        embeddings = self._transformer.encode(text_list, convert_to_numpy=True)\n","        print(' done')\n","        return embeddings"],"metadata":{"id":"bJfXaXjV8Ipf","executionInfo":{"status":"ok","timestamp":1737755472707,"user_tz":-60,"elapsed":14,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["## Semantic Searcher"],"metadata":{"id":"SDuRJDRl8Rym"}},{"cell_type":"code","source":["class SemanticSearcher:\n","\n","    def __init__(self, vectordb_name:Config.VECTOR_DB_NAMES, similarity_metric_name:Config.SIMILARITY_METRIC_NAMES) -> None:\n","        self.vectordb_name = vectordb_name\n","        self.similarity_metric_name = similarity_metric_name\n","        if self.vectordb_name == Config.VECTOR_DB_NAMES.FAISS:\n","            if self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.L2:\n","                self._engine = faiss.IndexFlatL2(768)\n","            elif self.similarity_metric_name in [Config.SIMILARITY_METRIC_NAMES.IP, Config.SIMILARITY_METRIC_NAMES.CS]:\n","                self._engine = faiss.IndexFlatIP(768)\n","        elif self.vectordb_name == Config.VECTOR_DB_NAMES.SCANN:\n","            self._engine:Any = None\n","\n","    def index(self, passage_embeddings:np.ndarray) -> None:\n","        if self.vectordb_name == Config.VECTOR_DB_NAMES.FAISS:\n","            if self.similarity_metric_name in [Config.SIMILARITY_METRIC_NAMES.L2, Config.SIMILARITY_METRIC_NAMES.IP]:\n","                self._engine.add(passage_embeddings)\n","            elif self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.CS:\n","                normalized_passage_embeddings = passage_embeddings / np.linalg.norm(passage_embeddings, axis=1, keepdims=True)\n","                self._engine.add(normalized_passage_embeddings)\n","        elif self.vectordb_name == Config.VECTOR_DB_NAMES.SCANN:\n","            if self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.L2:\n","                self._engine = scann.scann_ops_pybind.builder(passage_embeddings, RETRIEVAL_CAPACITY, 'squared_l2').score_brute_force().build()\n","            elif self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.IP:\n","                self._engine = scann.scann_ops_pybind.builder(passage_embeddings, RETRIEVAL_CAPACITY, 'dot_product').score_brute_force().build()\n","            elif self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.CS:\n","                normalized_passage_embeddings = passage_embeddings / np.linalg.norm(passage_embeddings, axis=1, keepdims=True)\n","                self._engine = scann.scann_ops_pybind.builder(normalized_passage_embeddings, RETRIEVAL_CAPACITY, 'dot_product').score_brute_force().build()\n","\n","    def search(self, query_embeddings:np.ndarray) -> np.ndarray:\n","        if self.vectordb_name == Config.VECTOR_DB_NAMES.FAISS:\n","            if self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.L2:\n","                retrieved_passage_indices = self._engine.search(query_embeddings, RETRIEVAL_CAPACITY)[1]\n","            elif self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.IP:\n","                retrieved_passage_indices = self._engine.search(query_embeddings, RETRIEVAL_CAPACITY)[1]\n","            elif self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.CS:\n","                normalized_query_embeddings = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n","                retrieved_passage_indices = self._engine.search(normalized_query_embeddings, RETRIEVAL_CAPACITY)[1]\n","        elif self.vectordb_name == Config.VECTOR_DB_NAMES.SCANN:\n","            retrieved_passage_index_matrix = list[list[int]]()\n","            if self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.L2:\n","                for i in range(query_embeddings.shape[0]):\n","                    retrieved_passage_index_list = self._engine.search(query_embeddings[i, :])[0]\n","                    retrieved_passage_index_matrix.append(retrieved_passage_index_list)\n","            elif self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.IP:\n","                for i in range(query_embeddings.shape[0]):\n","                    retrieved_passage_index_list = self._engine.search(query_embeddings[i, :])[0]\n","                    retrieved_passage_index_matrix.append(retrieved_passage_index_list)\n","            elif self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.CS:\n","                normalized_query_embeddings = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n","                for i in range(normalized_query_embeddings.shape[0]):\n","                    retrieved_passage_index_list = self._engine.search(normalized_query_embeddings[i, :])[0]\n","                    retrieved_passage_index_matrix.append(retrieved_passage_index_list)\n","            retrieved_passage_indices = np.array(retrieved_passage_index_matrix)\n","        return retrieved_passage_indices"],"metadata":{"id":"x2VRsBtT8Q63","executionInfo":{"status":"ok","timestamp":1737755472707,"user_tz":-60,"elapsed":13,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["## Syntactic Searcher"],"metadata":{"id":"4m9pIpKE8LW3"}},{"cell_type":"code","source":["class SyntacticSearcher:\n","\n","    def __init__(self, scorer_name:Config.SCORER_NAMES, aggregator_name:Config.AGGREGATOR_NAMES) -> None:\n","        self.scorer_name = scorer_name\n","        self.aggregator_name = aggregator_name\n","        if scorer_name == Config.SCORER_NAMES.SIMPLE:\n","            self._scorer:Callable[[str, str, float], float] = lambda query_entity, passage_entity, score_cutoff : float(query_entity == passage_entity)\n","        elif scorer_name == Config.SCORER_NAMES.LEVENSHTEIN:\n","            self._scorer:Callable[[str, str, float], float] = Levenshtein.distance\n","        elif scorer_name == Config.SCORER_NAMES.JARO_WINKLER:\n","            self._scorer:Callable[[str, str, float], float] = Levenshtein.jaro_winkler\n","        if aggregator_name == Config.AGGREGATOR_NAMES.MIN_MIN_0:\n","            self._aggregator:Callable[[np.ndarray], torch.Tensor] = lambda entity_similarities: torch.tensor(entity_similarities, device=DEVICE).min(dim=0)[0].min()\n","        elif aggregator_name == Config.AGGREGATOR_NAMES.MIN_AVG_0:\n","            self._aggregator:Callable[[np.ndarray], torch.Tensor] = lambda entity_similarities: torch.tensor(entity_similarities, device=DEVICE).min(dim=0)[0].mean()\n","        elif aggregator_name == Config.AGGREGATOR_NAMES.MIN_MAX_0:\n","            self._aggregator:Callable[[np.ndarray], torch.Tensor] = lambda entity_similarities: torch.tensor(entity_similarities, device=DEVICE).min(dim=0)[0].max()\n","        elif aggregator_name == Config.AGGREGATOR_NAMES.AVG_MIN_0:\n","            self._aggregator:Callable[[np.ndarray], torch.Tensor] = lambda entity_similarities: torch.tensor(entity_similarities, device=DEVICE).mean(dim=0).min()\n","        elif aggregator_name == Config.AGGREGATOR_NAMES.AVG_AVG_0:\n","            self._aggregator:Callable[[np.ndarray], torch.Tensor] = lambda entity_similarities: torch.tensor(entity_similarities, device=DEVICE).mean(dim=0).mean()\n","        elif aggregator_name == Config.AGGREGATOR_NAMES.AVG_MAX_0:\n","            self._aggregator:Callable[[np.ndarray], torch.Tensor] = lambda entity_similarities: torch.tensor(entity_similarities, device=DEVICE).mean(dim=0).max()\n","        elif aggregator_name == Config.AGGREGATOR_NAMES.MAX_MIN_0:\n","            self._aggregator:Callable[[np.ndarray], torch.Tensor] = lambda entity_similarities: torch.tensor(entity_similarities, device=DEVICE).max(dim=0)[0].min()\n","        elif aggregator_name == Config.AGGREGATOR_NAMES.MAX_AVG_0:\n","            self._aggregator:Callable[[np.ndarray], torch.Tensor] = lambda entity_similarities: torch.tensor(entity_similarities, device=DEVICE).max(dim=0)[0].mean()\n","        elif aggregator_name == Config.AGGREGATOR_NAMES.MAX_MAX_0:\n","            self._aggregator:Callable[[np.ndarray], torch.Tensor] = lambda entity_similarities: torch.tensor(entity_similarities, device=DEVICE).max(dim=0)[0].max()\n","        elif aggregator_name == Config.AGGREGATOR_NAMES.MIN_MIN_1:\n","            self._aggregator:Callable[[np.ndarray], torch.Tensor] = lambda entity_similarities: torch.tensor(entity_similarities, device=DEVICE).min(dim=1)[0].min()\n","        elif aggregator_name == Config.AGGREGATOR_NAMES.MIN_AVG_1:\n","            self._aggregator:Callable[[np.ndarray], torch.Tensor] = lambda entity_similarities: torch.tensor(entity_similarities, device=DEVICE).min(dim=1)[0].mean()\n","        elif aggregator_name == Config.AGGREGATOR_NAMES.MIN_MAX_1:\n","            self._aggregator:Callable[[np.ndarray], torch.Tensor] = lambda entity_similarities: torch.tensor(entity_similarities, device=DEVICE).min(dim=1)[0].max()\n","        elif aggregator_name == Config.AGGREGATOR_NAMES.AVG_MIN_1:\n","            self._aggregator:Callable[[np.ndarray], torch.Tensor] = lambda entity_similarities: torch.tensor(entity_similarities, device=DEVICE).mean(dim=1).min()\n","        elif aggregator_name == Config.AGGREGATOR_NAMES.AVG_AVG_1:\n","            self._aggregator:Callable[[np.ndarray], torch.Tensor] = lambda entity_similarities: torch.tensor(entity_similarities, device=DEVICE).mean(dim=1).mean()\n","        elif aggregator_name == Config.AGGREGATOR_NAMES.AVG_MAX_1:\n","            self._aggregator:Callable[[np.ndarray], torch.Tensor] = lambda entity_similarities: torch.tensor(entity_similarities, device=DEVICE).mean(dim=1).max()\n","        elif aggregator_name == Config.AGGREGATOR_NAMES.MAX_MIN_1:\n","            self._aggregator:Callable[[np.ndarray], torch.Tensor] = lambda entity_similarities: torch.tensor(entity_similarities, device=DEVICE).max(dim=1)[0].min()\n","        elif aggregator_name == Config.AGGREGATOR_NAMES.MAX_AVG_1:\n","            self._aggregator:Callable[[np.ndarray], torch.Tensor] = lambda entity_similarities: torch.tensor(entity_similarities, device=DEVICE).max(dim=1)[0].mean()\n","        elif aggregator_name == Config.AGGREGATOR_NAMES.MAX_MAX_1:\n","            self._aggregator:Callable[[np.ndarray], torch.Tensor] = lambda entity_similarities: torch.tensor(entity_similarities, device=DEVICE).max(dim=1)[0].max()\n","        self._passage_augmentation_list:list[dict[str, dict[str, int]]] = None\n","\n","    def index(self, passage_augmentation_list:list[dict[str, dict[str, int]]]) -> None:\n","        self._passage_augmentation_list = passage_augmentation_list\n","\n","    def search(self, query_augmentation_list:list[dict[str, dict[str, int]]]) -> tuple[np.ndarray, np.ndarray]:\n","        distances = torch.ones((len(query_augmentation_list), len(self._passage_augmentation_list)), device=DEVICE)\n","        with tqdm(total=len(query_augmentation_list) * len(self._passage_augmentation_list), desc='Computing Syntactic Similarities') as pbar:\n","            for i in range(len(query_augmentation_list)):\n","                for j in range(len(self._passage_augmentation_list)):\n","                    common_key_set = query_augmentation_list[i].keys() & self._passage_augmentation_list[j].keys()\n","                    if len(common_key_set) > 0:\n","                        similarities = torch.zeros(len(common_key_set), device=DEVICE)\n","                        for k, key in enumerate(common_key_set):\n","                            query_entity_array = np.array(list(query_augmentation_list[i][key].keys()))\n","                            query_frequency_array = np.array(list(query_augmentation_list[i][key].values()))\n","                            passage_entity_array = np.array(list(self._passage_augmentation_list[j][key].keys()))\n","                            passage_frequency_array = np.array(list(self._passage_augmentation_list[j][key].values()))\n","                            entity_similarities = cdist(query_entity_array, passage_entity_array, scorer=self._scorer)\n","                            entity_similarities = entity_similarities * query_frequency_array.reshape(-1, 1) / query_frequency_array.max()\n","                            entity_similarities = entity_similarities * passage_frequency_array.reshape(1, -1) / passage_frequency_array.max()\n","                            similarities[k] = self._aggregator(entity_similarities)\n","                        distances[i, j] = 1.0 - similarities.mean() * len(common_key_set) / len(query_augmentation_list[i])\n","                    pbar.update(1)\n","        print()\n","        retrieved_passage_indices = torch.topk(distances, k=min(RETRIEVAL_CAPACITY, distances.shape[1]), dim=1, largest=False).indices\n","        return retrieved_passage_indices.cpu().numpy()"],"metadata":{"id":"sq1ot6qc8Nvg","executionInfo":{"status":"ok","timestamp":1737755472708,"user_tz":-60,"elapsed":11,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["## BM25 Searcher"],"metadata":{"id":"90AuD6qXHUTC"}},{"cell_type":"code","source":["class BM25Searcher:\n","\n","    def __init__(self, tokenizer_name:Config.TOKENIZER_NAMES) -> None:\n","        spacy.prefer_gpu()\n","        self.tokenizer_name = tokenizer_name\n","        self._nlp = spacy.load('en_core_web_sm')\n","        if tokenizer_name == Config.TOKENIZER_NAMES.SIMPLE:\n","            self._tokenizer:Callable[[str], list[str]] = lambda text: text.split(' ')\n","        elif tokenizer_name == Config.TOKENIZER_NAMES.LEMMA:\n","            self._tokenizer:Callable[[str], list[str]] = lambda text: [token.lemma_ for token in self._nlp(text)]\n","        self._engine:BM25Okapi = None\n","        self._passage_list:list[str] = None\n","\n","    def index(self, passage_list:list[str]) -> None:\n","        tokenized_passage_list = list[list[str]]()\n","        with tqdm(total=len(passage_list), desc='Tokenizing Passages') as pbar:\n","            for passage in passage_list:\n","                tokenized_passage_list.append(self._tokenizer(passage))\n","                pbar.update(1)\n","        print()\n","        self._engine = BM25Okapi(tokenized_passage_list)\n","        self._passage_list = passage_list\n","\n","    def search(self, query_list:list[str]) -> tuple[np.ndarray, np.ndarray]:\n","        tokenized_query_list = list[list[str]]()\n","        with tqdm(total=len(query_list), desc='Tokenizing Queries') as pbar:\n","            for query in query_list:\n","                tokenized_query_list.append(self._tokenizer(query))\n","                pbar.update(1)\n","        print()\n","        distances = torch.ones((len(query_list), len(self._passage_list)), device=DEVICE)\n","        with tqdm(total=len(tokenized_query_list), desc='Computing BM25 Scores') as pbar:\n","            for i in range(len(query_list)):\n","                distances[i, :] = 1.0 - torch.tensor(self._engine.get_scores(tokenized_query_list[i]), device=DEVICE)\n","                pbar.update(1)\n","        print()\n","        retrieved_passage_indices = torch.topk(distances, k=min(RETRIEVAL_CAPACITY, distances.shape[1]), dim=1, largest=False).indices\n","        return retrieved_passage_indices.cpu().numpy()"],"metadata":{"id":"xNQK-V9aAGE0","executionInfo":{"status":"ok","timestamp":1737755472708,"user_tz":-60,"elapsed":9,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aHWYgScvAKuE"},"source":["# Operation"]},{"cell_type":"markdown","source":["## Baselines"],"metadata":{"id":"-wFyLvACbrWL"}},{"cell_type":"code","source":["transformer = Transformer(Config.TRANSFORMER_MODEL_NAMES.ALL_MPNET_BASE_V2)\n","ms_marco_dataset = Dataset('ms-marco-spacy-llama8b')\n","hotpot_qa_dataset = Dataset('hotpot-qa-spacy-llama8b')\n","print()\n","print(ms_marco_dataset)\n","print()\n","print(hotpot_qa_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fAIbrncLPPy-","executionInfo":{"status":"ok","timestamp":1737755476164,"user_tz":-60,"elapsed":1714,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}},"outputId":"225a306e-38a7-47a3-c7ac-1030f0696578"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","names -> file: ms-marco-spacy-llama8b, dataset: DATASET_NAMES.MS_MARCO\n","passages -> total_: 814, minimum_length: 88, average_length: 420, maximum_length: 922\n","queries -> total_: 100, minimum_length: 11, average_length: 34, maximum_length: 76\n","augmentations -> total_queries_augmented_with_spacy_ner: 100, total_queries_augmented_with_llama3-8b-8192_keyword_and_topic_extraction: 100\n","relations -> minimum_related_passages: 4, average_related_passages: 8, maximum_related_passages: 10\n","learning -> total_queries_in_train_set: 80, total_queries_in_validation_set: 10, total_queries_in_test_set: 10\n","\n","names -> file: hotpot-qa-spacy-llama8b, dataset: DATASET_NAMES.HOTPOT_QA\n","passages -> total_: 1000, minimum_length: 80, average_length: 598, maximum_length: 8307\n","queries -> total_: 100, minimum_length: 40, average_length: 111, maximum_length: 418\n","augmentations -> total_queries_augmented_with_spacy_ner: 100, total_queries_augmented_with_llama3-8b-8192_keyword_and_topic_extraction: 38\n","relations -> minimum_related_passages: 10, average_related_passages: 10, maximum_related_passages: 10\n","learning -> total_queries_in_train_set: 80, total_queries_in_validation_set: 10, total_queries_in_test_set: 10\n"]}]},{"cell_type":"code","source":["print('B1: Syntactic Search I')\n","ms_marco_syntactic_searcher = SyntacticSearcher(Config.SCORER_NAMES.JARO_WINKLER, Config.AGGREGATOR_NAMES.MAX_AVG_1)\n","ms_marco_syntactic_searcher.index(ms_marco_dataset.passage_augmentation_list)\n","ms_marco_b1_retrieved_passage_indices = ms_marco_syntactic_searcher.search(ms_marco_dataset.query_augmentation_list)\n","ms_marco_dataset.print_metrics(list(range(len(ms_marco_dataset.query_list))), ms_marco_b1_retrieved_passage_indices)\n","\n","print()\n","print('B2: Syntactic Search II')\n","ms_marco_bm25_searcher = BM25Searcher(Config.TOKENIZER_NAMES.LEMMA)\n","ms_marco_bm25_searcher.index(ms_marco_dataset.passage_list)\n","ms_marco_b2_retrieved_passage_indices = ms_marco_bm25_searcher.search(ms_marco_dataset.query_list)\n","ms_marco_dataset.print_metrics(list(range(len(ms_marco_dataset.query_list))), ms_marco_b2_retrieved_passage_indices)\n","\n","print()\n","print('B3: Semantic Search')\n","ms_marco_semantic_searcher = SemanticSearcher(Config.VECTOR_DB_NAMES.FAISS, Config.SIMILARITY_METRIC_NAMES.CS)\n","ms_marco_b3_passage_embeddings = transformer.embed(ms_marco_dataset.passage_list)\n","ms_marco_b3_query_embeddings = transformer.embed(ms_marco_dataset.query_list)\n","ms_marco_semantic_searcher.index(ms_marco_b3_passage_embeddings)\n","ms_marco_b3_retrieved_passage_indices = ms_marco_semantic_searcher.search(ms_marco_b3_query_embeddings)\n","ms_marco_dataset.print_metrics(list(range(len(ms_marco_dataset.query_list))), ms_marco_b3_retrieved_passage_indices)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CnLU22N82G7j","executionInfo":{"status":"ok","timestamp":1737755573405,"user_tz":-60,"elapsed":87659,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}},"outputId":"8caa1055-616e-48a4-b583-85c02acf9856"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["B1: Syntactic Search I\n"]},{"output_type":"stream","name":"stderr","text":["Computing Syntactic Similarities: 100%|██████████| 81400/81400 [00:48<00:00, 1690.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Dataset Name -> DATASET_NAMES.MS_MARCO\n","Recall -> 1: 11.02% | 2: 21.94% | 3: 32.51% | 4: 41.12% | 5: 48.87% | 6: 55.12% | 7: 60.08% | 8: 63.95% | 9: 66.94% | 10: 69.10% | 11: 70.37% | 12: 72.44% | 13: 73.48% | 14: 74.58% | 15: 75.59% | 16: 76.58% | 17: 76.93% | 18: 77.60% | 19: 78.23% | 20: 78.58% | 21: 79.05% | 22: 79.45% | 23: 79.78% | 24: 79.89% | 25: 80.00% | 26: 80.12% | 27: 80.45% | 28: 80.55% | 29: 80.82% | 30: 81.06% | 31: 81.57% | 32: 81.91% | 33: 82.11% | 34: 82.47% | 35: 82.63% | 36: 82.63% | 37: 82.89% | 38: 83.00% | 39: 83.14% | 40: 83.82% | 41: 84.05% | 42: 84.37% | 43: 84.48% | 44: 84.48% | 45: 84.58% | 46: 84.58% | 47: 84.69% | 48: 84.69% | 49: 84.95% | 50: 85.37% | 51: 85.49% | 52: 85.80% | 53: 86.16% | 54: 86.33% | 55: 86.33% | 56: 86.33% | 57: 86.64% | 58: 86.64% | 59: 86.64% | 60: 86.64% | 61: 86.64% | 62: 86.78% | 63: 86.91% | 64: 86.91% | 65: 87.17% | 66: 87.34% | 67: 87.61% | 68: 87.71% | 69: 87.83% | 70: 87.83% | 71: 87.94% | 72: 88.30% | 73: 88.30% | 74: 88.30% | 75: 88.50% | 76: 88.62% | 77: 88.62% | 78: 88.62% | 79: 88.95% | 80: 88.95% | 81: 89.09% | 82: 89.09% | 83: 89.09% | 84: 89.09% | 85: 89.19% | 86: 89.19% | 87: 89.36% | 88: 89.36% | 89: 89.36% | 90: 89.36% | 91: 89.36% | 92: 89.48% | 93: 89.48% | 94: 89.75% | 95: 89.75% | 96: 90.08% | 97: 90.08% | 98: 90.08% | 99: 90.08% | 100: 90.08%\n","Cluster Recall -> 4: 100.00% | 5: 70.00% | 6: 61.67% | 7: 69.05% | 8: 57.95% | 9: 69.63% | 10: 68.42%\n","MRR -> optimistic: 91.68% | pessimistic: 36.91%\n","\n","B2: Syntactic Search II\n"]},{"output_type":"stream","name":"stderr","text":["Tokenizing Passages: 100%|██████████| 814/814 [00:30<00:00, 26.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["Tokenizing Queries: 100%|██████████| 100/100 [00:01<00:00, 65.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["Computing BM25 Scores: 100%|██████████| 100/100 [00:00<00:00, 726.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Dataset Name -> DATASET_NAMES.MS_MARCO\n","Recall -> 1: 12.32% | 2: 23.39% | 3: 33.86% | 4: 43.92% | 5: 52.71% | 6: 61.04% | 7: 67.63% | 8: 71.86% | 9: 74.54% | 10: 76.11% | 11: 77.54% | 12: 78.43% | 13: 79.10% | 14: 79.86% | 15: 80.36% | 16: 80.86% | 17: 81.10% | 18: 81.44% | 19: 81.87% | 20: 81.97% | 21: 81.97% | 22: 82.07% | 23: 82.07% | 24: 82.41% | 25: 82.63% | 26: 82.74% | 27: 82.96% | 28: 83.16% | 29: 83.26% | 30: 83.39% | 31: 83.51% | 32: 83.51% | 33: 83.61% | 34: 83.61% | 35: 83.71% | 36: 83.71% | 37: 83.81% | 38: 83.81% | 39: 84.14% | 40: 84.24% | 41: 84.24% | 42: 84.38% | 43: 84.38% | 44: 84.38% | 45: 84.38% | 46: 84.48% | 47: 84.48% | 48: 84.48% | 49: 84.58% | 50: 84.58% | 51: 84.68% | 52: 84.95% | 53: 84.95% | 54: 84.95% | 55: 84.95% | 56: 84.95% | 57: 84.95% | 58: 85.21% | 59: 85.21% | 60: 85.21% | 61: 85.21% | 62: 85.34% | 63: 85.45% | 64: 85.45% | 65: 85.45% | 66: 85.45% | 67: 85.45% | 68: 85.45% | 69: 85.56% | 70: 85.67% | 71: 85.67% | 72: 85.67% | 73: 85.67% | 74: 85.67% | 75: 85.67% | 76: 85.78% | 77: 85.93% | 78: 85.93% | 79: 85.93% | 80: 85.93% | 81: 85.93% | 82: 85.93% | 83: 85.93% | 84: 85.93% | 85: 85.93% | 86: 85.93% | 87: 85.93% | 88: 85.93% | 89: 85.93% | 90: 85.93% | 91: 86.05% | 92: 86.05% | 93: 86.05% | 94: 86.05% | 95: 86.05% | 96: 86.16% | 97: 86.16% | 98: 86.16% | 99: 86.16% | 100: 86.16%\n","Cluster Recall -> 4: 75.00% | 5: 60.00% | 6: 85.00% | 7: 72.62% | 8: 78.98% | 9: 75.19% | 10: 62.63%\n","MRR -> optimistic: 96.20% | pessimistic: 45.46%\n","\n","B3: Semantic Search\n","Embedding ... done\n","Embedding ... done\n","Dataset Name -> DATASET_NAMES.MS_MARCO\n","Recall -> 1: 12.81% | 2: 25.43% | 3: 38.24% | 4: 51.06% | 5: 63.50% | 6: 74.43% | 7: 83.81% | 8: 91.10% | 9: 95.69% | 10: 97.30% | 11: 97.51% | 12: 97.83% | 13: 98.06% | 14: 98.16% | 15: 98.30% | 16: 98.30% | 17: 98.54% | 18: 98.74% | 19: 98.74% | 20: 98.74% | 21: 98.74% | 22: 98.74% | 23: 98.94% | 24: 99.05% | 25: 99.05% | 26: 99.19% | 27: 99.19% | 28: 99.19% | 29: 99.30% | 30: 99.40% | 31: 99.40% | 32: 99.40% | 33: 99.40% | 34: 99.40% | 35: 99.40% | 36: 99.40% | 37: 99.40% | 38: 99.40% | 39: 99.40% | 40: 99.40% | 41: 99.40% | 42: 99.40% | 43: 99.40% | 44: 99.40% | 45: 99.40% | 46: 99.40% | 47: 99.40% | 48: 99.40% | 49: 99.40% | 50: 99.40% | 51: 99.40% | 52: 99.40% | 53: 99.40% | 54: 99.40% | 55: 99.40% | 56: 99.40% | 57: 99.40% | 58: 99.40% | 59: 99.40% | 60: 99.40% | 61: 99.40% | 62: 99.40% | 63: 99.40% | 64: 99.40% | 65: 99.40% | 66: 99.40% | 67: 99.40% | 68: 99.40% | 69: 99.40% | 70: 99.40% | 71: 99.40% | 72: 99.40% | 73: 99.40% | 74: 99.40% | 75: 99.40% | 76: 99.40% | 77: 99.40% | 78: 99.40% | 79: 99.40% | 80: 99.40% | 81: 99.40% | 82: 99.40% | 83: 99.40% | 84: 99.40% | 85: 99.40% | 86: 99.40% | 87: 99.40% | 88: 99.40% | 89: 99.40% | 90: 99.40% | 91: 99.40% | 92: 99.40% | 93: 99.40% | 94: 99.40% | 95: 99.40% | 96: 99.40% | 97: 99.40% | 98: 99.51% | 99: 99.51% | 100: 99.51%\n","Cluster Recall -> 4: 100.00% | 5: 96.67% | 6: 98.33% | 7: 97.62% | 8: 96.59% | 9: 97.41% | 10: 95.79%\n","MRR -> optimistic: 100.00% | pessimistic: 90.41%\n"]}]},{"cell_type":"code","source":["print('B1: Syntactic Search I')\n","hotpot_qa_syntactic_searcher = SyntacticSearcher(Config.SCORER_NAMES.JARO_WINKLER, Config.AGGREGATOR_NAMES.MAX_AVG_1)\n","hotpot_qa_syntactic_searcher.index(hotpot_qa_dataset.passage_augmentation_list)\n","hotpot_qa_b1_retrieved_passage_indices = hotpot_qa_syntactic_searcher.search(hotpot_qa_dataset.query_augmentation_list)\n","hotpot_qa_dataset.print_metrics(list(range(len(hotpot_qa_dataset.query_list))), hotpot_qa_b1_retrieved_passage_indices)\n","\n","print()\n","print('B2: Syntactic Search II')\n","hotpot_qa_bm25_searcher = BM25Searcher(Config.TOKENIZER_NAMES.LEMMA)\n","hotpot_qa_bm25_searcher.index(hotpot_qa_dataset.passage_list)\n","hotpot_qa_b2_retrieved_passage_indices = hotpot_qa_bm25_searcher.search(hotpot_qa_dataset.query_list)\n","hotpot_qa_dataset.print_metrics(list(range(len(hotpot_qa_dataset.query_list))), hotpot_qa_b2_retrieved_passage_indices)\n","\n","print()\n","print('B3: Semantic Search')\n","hotpot_qa_semantic_searcher = SemanticSearcher(Config.VECTOR_DB_NAMES.FAISS, Config.SIMILARITY_METRIC_NAMES.CS)\n","hotpot_qa_b3_passage_embeddings = transformer.embed(hotpot_qa_dataset.passage_list)\n","hotpot_qa_b3_query_embeddings = transformer.embed(hotpot_qa_dataset.query_list)\n","hotpot_qa_semantic_searcher.index(hotpot_qa_b3_passage_embeddings)\n","hotpot_qa_b3_retrieved_passage_indices = hotpot_qa_semantic_searcher.search(hotpot_qa_b3_query_embeddings)\n","hotpot_qa_dataset.print_metrics(list(range(len(hotpot_qa_dataset.query_list))), hotpot_qa_b3_retrieved_passage_indices)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rorcajoVPBm6","executionInfo":{"status":"ok","timestamp":1737757219855,"user_tz":-60,"elapsed":89262,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}},"outputId":"3e793b89-f90c-43b6-849d-a4a6d9ee013b"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["B1: Syntactic Search I\n"]},{"output_type":"stream","name":"stderr","text":["Computing Syntactic Similarities: 100%|██████████| 100000/100000 [00:46<00:00, 2145.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Dataset Name -> DATASET_NAMES.HOTPOT_QA\n","Recall -> 1: 8.00% | 2: 13.40% | 3: 18.10% | 4: 21.70% | 5: 25.10% | 6: 28.10% | 7: 30.40% | 8: 32.30% | 9: 34.00% | 10: 35.70% | 11: 36.50% | 12: 37.10% | 13: 38.00% | 14: 38.60% | 15: 39.50% | 16: 40.40% | 17: 41.20% | 18: 41.70% | 19: 42.40% | 20: 43.10% | 21: 43.70% | 22: 43.90% | 23: 44.50% | 24: 44.60% | 25: 45.20% | 26: 45.40% | 27: 45.60% | 28: 45.70% | 29: 46.20% | 30: 46.50% | 31: 46.80% | 32: 46.90% | 33: 47.20% | 34: 47.30% | 35: 47.60% | 36: 47.90% | 37: 48.00% | 38: 48.50% | 39: 48.80% | 40: 49.20% | 41: 49.50% | 42: 49.90% | 43: 50.10% | 44: 50.50% | 45: 50.60% | 46: 50.70% | 47: 51.00% | 48: 51.40% | 49: 51.40% | 50: 51.50% | 51: 51.50% | 52: 51.70% | 53: 52.00% | 54: 52.00% | 55: 52.60% | 56: 52.90% | 57: 53.10% | 58: 53.60% | 59: 54.00% | 60: 54.50% | 61: 54.60% | 62: 54.60% | 63: 54.70% | 64: 54.90% | 65: 55.20% | 66: 55.50% | 67: 55.60% | 68: 55.90% | 69: 55.90% | 70: 56.00% | 71: 56.00% | 72: 56.10% | 73: 56.30% | 74: 56.40% | 75: 56.70% | 76: 56.80% | 77: 57.10% | 78: 57.20% | 79: 57.40% | 80: 57.40% | 81: 57.40% | 82: 57.40% | 83: 57.40% | 84: 57.60% | 85: 57.80% | 86: 57.90% | 87: 58.20% | 88: 58.40% | 89: 58.50% | 90: 58.80% | 91: 58.80% | 92: 59.00% | 93: 59.00% | 94: 59.10% | 95: 59.20% | 96: 59.30% | 97: 59.40% | 98: 59.50% | 99: 59.80% | 100: 59.80%\n","Cluster Recall -> 10: 35.70%\n","MRR -> optimistic: 83.34% | pessimistic: 4.47%\n","\n","B2: Syntactic Search II\n"]},{"output_type":"stream","name":"stderr","text":["Tokenizing Passages: 100%|██████████| 1000/1000 [00:28<00:00, 35.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["Tokenizing Queries: 100%|██████████| 100/100 [00:01<00:00, 51.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["Computing BM25 Scores: 100%|██████████| 100/100 [00:00<00:00, 191.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Dataset Name -> DATASET_NAMES.HOTPOT_QA\n","Recall -> 1: 10.00% | 2: 19.80% | 3: 29.20% | 4: 38.30% | 5: 47.60% | 6: 55.80% | 7: 63.70% | 8: 71.30% | 9: 78.30% | 10: 83.60% | 11: 86.60% | 12: 88.20% | 13: 89.50% | 14: 90.00% | 15: 91.00% | 16: 91.60% | 17: 92.20% | 18: 92.50% | 19: 93.10% | 20: 93.80% | 21: 94.20% | 22: 94.30% | 23: 94.40% | 24: 94.60% | 25: 94.70% | 26: 94.90% | 27: 95.10% | 28: 95.30% | 29: 95.50% | 30: 95.60% | 31: 95.70% | 32: 95.80% | 33: 95.80% | 34: 95.90% | 35: 95.90% | 36: 95.90% | 37: 96.10% | 38: 96.20% | 39: 96.50% | 40: 96.60% | 41: 96.60% | 42: 96.60% | 43: 96.60% | 44: 96.60% | 45: 96.60% | 46: 96.60% | 47: 96.70% | 48: 96.70% | 49: 96.80% | 50: 97.00% | 51: 97.00% | 52: 97.00% | 53: 97.00% | 54: 97.10% | 55: 97.20% | 56: 97.20% | 57: 97.20% | 58: 97.30% | 59: 97.30% | 60: 97.40% | 61: 97.40% | 62: 97.40% | 63: 97.40% | 64: 97.50% | 65: 97.50% | 66: 97.50% | 67: 97.50% | 68: 97.50% | 69: 97.50% | 70: 97.50% | 71: 97.50% | 72: 97.50% | 73: 97.50% | 74: 97.50% | 75: 97.50% | 76: 97.50% | 77: 97.50% | 78: 97.50% | 79: 97.50% | 80: 97.50% | 81: 97.50% | 82: 97.50% | 83: 97.50% | 84: 97.50% | 85: 97.60% | 86: 97.60% | 87: 97.60% | 88: 97.60% | 89: 97.60% | 90: 97.60% | 91: 97.60% | 92: 97.60% | 93: 97.60% | 94: 97.60% | 95: 97.60% | 96: 97.60% | 97: 97.60% | 98: 97.60% | 99: 97.60% | 100: 97.60%\n","Cluster Recall -> 10: 83.60%\n","MRR -> optimistic: 100.00% | pessimistic: 61.53%\n","\n","B3: Semantic Search\n","Embedding ... done\n","Embedding ... done\n","Dataset Name -> DATASET_NAMES.HOTPOT_QA\n","Recall -> 1: 9.60% | 2: 18.80% | 3: 26.20% | 4: 33.00% | 5: 39.00% | 6: 44.30% | 7: 48.70% | 8: 53.00% | 9: 57.20% | 10: 60.50% | 11: 62.20% | 12: 63.30% | 13: 64.20% | 14: 65.00% | 15: 66.20% | 16: 66.90% | 17: 67.60% | 18: 68.40% | 19: 69.00% | 20: 69.70% | 21: 70.00% | 22: 70.40% | 23: 71.20% | 24: 71.70% | 25: 72.20% | 26: 72.40% | 27: 73.20% | 28: 74.10% | 29: 75.00% | 30: 75.50% | 31: 76.00% | 32: 76.10% | 33: 76.80% | 34: 77.10% | 35: 77.30% | 36: 77.60% | 37: 77.80% | 38: 78.20% | 39: 78.50% | 40: 78.70% | 41: 79.10% | 42: 79.40% | 43: 79.60% | 44: 79.70% | 45: 80.40% | 46: 80.80% | 47: 80.90% | 48: 81.10% | 49: 81.30% | 50: 81.40% | 51: 81.60% | 52: 81.80% | 53: 82.10% | 54: 82.30% | 55: 82.30% | 56: 82.50% | 57: 82.60% | 58: 82.70% | 59: 82.90% | 60: 83.00% | 61: 83.60% | 62: 83.90% | 63: 84.10% | 64: 84.20% | 65: 84.20% | 66: 84.30% | 67: 84.50% | 68: 84.70% | 69: 84.80% | 70: 85.10% | 71: 85.20% | 72: 85.50% | 73: 85.70% | 74: 85.70% | 75: 85.90% | 76: 86.00% | 77: 86.00% | 78: 86.20% | 79: 86.30% | 80: 86.40% | 81: 86.60% | 82: 86.70% | 83: 86.70% | 84: 86.80% | 85: 86.90% | 86: 86.90% | 87: 87.00% | 88: 87.00% | 89: 87.10% | 90: 87.20% | 91: 87.40% | 92: 87.40% | 93: 87.40% | 94: 87.40% | 95: 87.70% | 96: 87.70% | 97: 87.70% | 98: 87.70% | 99: 87.70% | 100: 88.00%\n","Cluster Recall -> 10: 60.50%\n","MRR -> optimistic: 97.83% | pessimistic: 24.13%\n"]}]},{"cell_type":"markdown","source":["## Improvements"],"metadata":{"id":"I6-WSuvnegif"}},{"cell_type":"code","source":["ms_marco_transformer = Transformer(Config.TRANSFORMER_MODEL_NAMES.ALL_MPNET_BASE_V2)\n","ms_marco_semantic_searcher = SemanticSearcher(Config.VECTOR_DB_NAMES.FAISS, Config.SIMILARITY_METRIC_NAMES.CS)\n","\n","ms_marco_dataset = Dataset('ms-marco-no-augmentation')\n","print()\n","print(ms_marco_dataset)\n","\n","ms_marco_b3_passage_embeddings = ms_marco_transformer.embed(ms_marco_dataset.passage_list)\n","ms_marco_b3_query_embeddings = ms_marco_transformer.embed(ms_marco_dataset.query_list)\n","ms_marco_semantic_searcher.index(ms_marco_b3_passage_embeddings)\n","ms_marco_b3_retrieved_passage_indices = ms_marco_semantic_searcher.search(ms_marco_b3_query_embeddings)\n","\n","print()\n","print('B3: Semantic Search (MS-Marco, Train)')\n","ms_marco_train_query_index_list = list(range(len(ms_marco_dataset.train_set)))\n","ms_marco_dataset.print_metrics(ms_marco_train_query_index_list, ms_marco_b3_retrieved_passage_indices[ms_marco_train_query_index_list, :])\n","\n","print()\n","print('B3: Semantic Search (MS-Marco, Test)')\n","ms_marco_test_query_index_list = list(range(len(ms_marco_dataset.test_set)))\n","ms_marco_dataset.print_metrics(ms_marco_test_query_index_list, ms_marco_b3_retrieved_passage_indices[ms_marco_test_query_index_list, :])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ME2JMjLPy8B","executionInfo":{"status":"ok","timestamp":1737759701001,"user_tz":-60,"elapsed":62255,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}},"outputId":"4afe69b5-09f7-4a60-c980-93d269cb025a"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","names -> file: ms-marco-no-augmentation, dataset: DATASET_NAMES.MS_MARCO\n","passages -> total_: 8209, minimum_length: 59, average_length: 418, maximum_length: 922\n","queries -> total_: 1000, minimum_length: 11, average_length: 34, maximum_length: 109\n","relations -> minimum_related_passages: 2, average_related_passages: 8, maximum_related_passages: 10\n","learning -> total_queries_in_train_set: 800, total_queries_in_validation_set: 100, total_queries_in_test_set: 100\n","Embedding ... done\n","Embedding ... done\n","\n","B3: Semantic Search (MS-Marco, Train)\n","Dataset Name -> DATASET_NAMES.MS_MARCO\n","Recall -> 1: 12.65% | 2: 25.26% | 3: 37.55% | 4: 49.41% | 5: 60.48% | 6: 70.27% | 7: 78.65% | 8: 85.84% | 9: 90.50% | 10: 92.62% | 11: 93.48% | 12: 94.19% | 13: 94.66% | 14: 95.06% | 15: 95.58% | 16: 95.94% | 17: 96.23% | 18: 96.53% | 19: 96.65% | 20: 96.77% | 21: 96.98% | 22: 97.08% | 23: 97.15% | 24: 97.30% | 25: 97.33% | 26: 97.46% | 27: 97.55% | 28: 97.62% | 29: 97.73% | 30: 97.76% | 31: 97.77% | 32: 97.85% | 33: 97.91% | 34: 97.99% | 35: 98.02% | 36: 98.03% | 37: 98.10% | 38: 98.10% | 39: 98.13% | 40: 98.19% | 41: 98.22% | 42: 98.29% | 43: 98.31% | 44: 98.34% | 45: 98.38% | 46: 98.38% | 47: 98.42% | 48: 98.44% | 49: 98.44% | 50: 98.46% | 51: 98.46% | 52: 98.47% | 53: 98.47% | 54: 98.49% | 55: 98.51% | 56: 98.53% | 57: 98.57% | 58: 98.59% | 59: 98.59% | 60: 98.60% | 61: 98.63% | 62: 98.63% | 63: 98.63% | 64: 98.63% | 65: 98.64% | 66: 98.64% | 67: 98.64% | 68: 98.68% | 69: 98.68% | 70: 98.68% | 71: 98.70% | 72: 98.72% | 73: 98.72% | 74: 98.72% | 75: 98.73% | 76: 98.76% | 77: 98.81% | 78: 98.84% | 79: 98.84% | 80: 98.84% | 81: 98.84% | 82: 98.86% | 83: 98.86% | 84: 98.87% | 85: 98.89% | 86: 98.89% | 87: 98.90% | 88: 98.92% | 89: 98.92% | 90: 98.92% | 91: 98.92% | 92: 98.92% | 93: 98.94% | 94: 98.94% | 95: 98.94% | 96: 98.94% | 97: 98.94% | 98: 98.94% | 99: 98.94% | 100: 98.94%\n","Cluster Recall -> 2: 100.00% | 3: 100.00% | 4: 96.67% | 5: 90.22% | 6: 91.67% | 7: 88.03% | 8: 89.45% | 9: 91.73% | 10: 88.25%\n","MRR -> optimistic: 98.90% | pessimistic: 79.10%\n","\n","B3: Semantic Search (MS-Marco, Test)\n","Dataset Name -> DATASET_NAMES.MS_MARCO\n","Recall -> 1: 12.45% | 2: 25.00% | 3: 37.45% | 4: 49.50% | 5: 60.98% | 6: 70.03% | 7: 77.88% | 8: 83.89% | 9: 89.24% | 10: 91.38% | 11: 92.05% | 12: 93.30% | 13: 93.96% | 14: 94.56% | 15: 95.28% | 16: 95.50% | 17: 95.71% | 18: 96.02% | 19: 96.12% | 20: 96.50% | 21: 96.85% | 22: 96.85% | 23: 96.96% | 24: 97.17% | 25: 97.27% | 26: 97.38% | 27: 97.38% | 28: 97.38% | 29: 97.38% | 30: 97.38% | 31: 97.38% | 32: 97.53% | 33: 97.53% | 34: 97.64% | 35: 97.64% | 36: 97.64% | 37: 97.76% | 38: 97.76% | 39: 97.76% | 40: 97.76% | 41: 97.76% | 42: 97.86% | 43: 97.86% | 44: 97.86% | 45: 97.86% | 46: 97.86% | 47: 97.86% | 48: 97.86% | 49: 97.86% | 50: 97.86% | 51: 97.86% | 52: 97.86% | 53: 97.86% | 54: 97.86% | 55: 97.86% | 56: 97.86% | 57: 97.86% | 58: 97.86% | 59: 97.86% | 60: 97.86% | 61: 98.10% | 62: 98.10% | 63: 98.10% | 64: 98.10% | 65: 98.21% | 66: 98.21% | 67: 98.21% | 68: 98.21% | 69: 98.21% | 70: 98.21% | 71: 98.32% | 72: 98.32% | 73: 98.32% | 74: 98.32% | 75: 98.32% | 76: 98.32% | 77: 98.42% | 78: 98.42% | 79: 98.42% | 80: 98.42% | 81: 98.42% | 82: 98.42% | 83: 98.42% | 84: 98.42% | 85: 98.42% | 86: 98.42% | 87: 98.42% | 88: 98.42% | 89: 98.42% | 90: 98.42% | 91: 98.42% | 92: 98.42% | 93: 98.42% | 94: 98.42% | 95: 98.42% | 96: 98.42% | 97: 98.42% | 98: 98.42% | 99: 98.42% | 100: 98.42%\n","Cluster Recall -> 4: 100.00% | 5: 97.78% | 6: 95.83% | 7: 85.71% | 8: 85.00% | 9: 90.88% | 10: 86.67%\n","MRR -> optimistic: 98.08% | pessimistic: 77.41%\n"]}]},{"cell_type":"code","source":["hotpot_qa_transformer = Transformer(Config.TRANSFORMER_MODEL_NAMES.ALL_MPNET_BASE_V2)\n","hotpot_qa_semantic_searcher = SemanticSearcher(Config.VECTOR_DB_NAMES.FAISS, Config.SIMILARITY_METRIC_NAMES.CS)\n","\n","hotpot_qa_dataset = Dataset('hotpot-qa-no-augmentation')\n","print()\n","print(hotpot_qa_dataset)\n","\n","hotpot_qa_b3_passage_embeddings = hotpot_qa_transformer.embed(hotpot_qa_dataset.passage_list)\n","hotpot_qa_b3_query_embeddings = hotpot_qa_transformer.embed(hotpot_qa_dataset.query_list)\n","hotpot_qa_semantic_searcher.index(hotpot_qa_b3_passage_embeddings)\n","hotpot_qa_b3_retrieved_passage_indices = hotpot_qa_semantic_searcher.search(hotpot_qa_b3_query_embeddings)\n","\n","print()\n","print('B3: Semantic Search (Hotpot-QA, Train)')\n","hotpot_qa_train_query_index_list = list(range(len(hotpot_qa_dataset.train_set)))\n","hotpot_qa_dataset.print_metrics(hotpot_qa_train_query_index_list, hotpot_qa_b3_retrieved_passage_indices[hotpot_qa_train_query_index_list, :])\n","\n","print()\n","print('B3: Semantic Search (Hotpot-QA, Test)')\n","hotpot_qa_test_query_index_list = list(range(len(hotpot_qa_dataset.test_set)))\n","hotpot_qa_dataset.print_metrics(hotpot_qa_test_query_index_list, hotpot_qa_b3_retrieved_passage_indices[hotpot_qa_test_query_index_list, :])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ap6ZAhTrP6ts","executionInfo":{"status":"ok","timestamp":1737760003172,"user_tz":-60,"elapsed":95759,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}},"outputId":"a61e8341-80cb-4de3-d8fe-0876637bc1fd"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","names -> file: hotpot-qa-no-augmentation, dataset: DATASET_NAMES.HOTPOT_QA\n","passages -> total_: 9913, minimum_length: 63, average_length: 567, maximum_length: 8307\n","queries -> total_: 1000, minimum_length: 32, average_length: 104, maximum_length: 542\n","relations -> minimum_related_passages: 1, average_related_passages: 10, maximum_related_passages: 10\n","learning -> total_queries_in_train_set: 800, total_queries_in_validation_set: 100, total_queries_in_test_set: 100\n","Embedding ... done\n","Embedding ... done\n","\n","B3: Semantic Search (Hotpot-QA, Train)\n","Dataset Name -> DATASET_NAMES.HOTPOT_QA\n","Recall -> 1: 9.27% | 2: 15.97% | 3: 21.22% | 4: 25.67% | 5: 29.54% | 6: 32.91% | 7: 35.62% | 8: 38.14% | 9: 40.51% | 10: 42.35% | 11: 43.89% | 12: 45.44% | 13: 46.86% | 14: 47.91% | 15: 48.92% | 16: 49.89% | 17: 50.68% | 18: 51.46% | 19: 52.13% | 20: 52.90% | 21: 53.54% | 22: 54.22% | 23: 54.86% | 24: 55.48% | 25: 55.98% | 26: 56.41% | 27: 56.94% | 28: 57.37% | 29: 57.72% | 30: 58.12% | 31: 58.52% | 32: 58.86% | 33: 59.17% | 34: 59.48% | 35: 59.81% | 36: 60.04% | 37: 60.31% | 38: 60.58% | 39: 60.83% | 40: 61.06% | 41: 61.41% | 42: 61.68% | 43: 61.91% | 44: 62.28% | 45: 62.59% | 46: 62.86% | 47: 62.96% | 48: 63.13% | 49: 63.37% | 50: 63.49% | 51: 63.64% | 52: 63.94% | 53: 64.16% | 54: 64.42% | 55: 64.61% | 56: 64.75% | 57: 65.00% | 58: 65.11% | 59: 65.24% | 60: 65.36% | 61: 65.57% | 62: 65.72% | 63: 65.92% | 64: 66.06% | 65: 66.21% | 66: 66.42% | 67: 66.54% | 68: 66.71% | 69: 66.81% | 70: 66.96% | 71: 67.16% | 72: 67.34% | 73: 67.50% | 74: 67.66% | 75: 67.85% | 76: 67.96% | 77: 68.04% | 78: 68.14% | 79: 68.24% | 80: 68.31% | 81: 68.35% | 82: 68.40% | 83: 68.47% | 84: 68.61% | 85: 68.69% | 86: 68.85% | 87: 69.04% | 88: 69.15% | 89: 69.36% | 90: 69.45% | 91: 69.60% | 92: 69.67% | 93: 69.82% | 94: 69.94% | 95: 70.04% | 96: 70.14% | 97: 70.19% | 98: 70.27% | 99: 70.37% | 100: 70.49%\n","Cluster Recall -> 1: 100.00% | 2: 87.50% | 3: 33.33% | 5: 40.00% | 6: 33.33% | 9: 44.44% | 10: 42.04%\n","MRR -> optimistic: 92.39% | pessimistic: 10.67%\n","\n","B3: Semantic Search (Hotpot-QA, Test)\n","Dataset Name -> DATASET_NAMES.HOTPOT_QA\n","Recall -> 1: 10.10% | 2: 16.80% | 3: 21.70% | 4: 26.80% | 5: 30.80% | 6: 34.20% | 7: 37.00% | 8: 38.90% | 9: 40.90% | 10: 42.90% | 11: 44.90% | 12: 45.50% | 13: 46.50% | 14: 47.90% | 15: 49.50% | 16: 50.60% | 17: 51.80% | 18: 52.60% | 19: 53.00% | 20: 53.40% | 21: 53.70% | 22: 54.10% | 23: 54.70% | 24: 55.30% | 25: 55.70% | 26: 56.20% | 27: 56.70% | 28: 57.20% | 29: 57.40% | 30: 57.80% | 31: 58.40% | 32: 58.80% | 33: 59.00% | 34: 59.40% | 35: 59.80% | 36: 60.10% | 37: 60.40% | 38: 60.40% | 39: 60.50% | 40: 60.70% | 41: 61.10% | 42: 61.20% | 43: 61.30% | 44: 61.50% | 45: 62.00% | 46: 62.60% | 47: 62.80% | 48: 62.90% | 49: 63.00% | 50: 63.10% | 51: 63.10% | 52: 63.60% | 53: 64.10% | 54: 64.20% | 55: 64.30% | 56: 64.30% | 57: 64.50% | 58: 64.60% | 59: 64.80% | 60: 64.80% | 61: 65.10% | 62: 65.20% | 63: 65.40% | 64: 65.50% | 65: 65.60% | 66: 65.80% | 67: 65.80% | 68: 65.80% | 69: 66.20% | 70: 66.30% | 71: 66.50% | 72: 66.60% | 73: 66.60% | 74: 66.60% | 75: 67.20% | 76: 67.20% | 77: 67.20% | 78: 67.20% | 79: 67.20% | 80: 67.30% | 81: 67.30% | 82: 67.30% | 83: 67.30% | 84: 67.40% | 85: 67.40% | 86: 67.60% | 87: 67.80% | 88: 67.90% | 89: 68.00% | 90: 68.00% | 91: 68.00% | 92: 68.00% | 93: 68.10% | 94: 68.10% | 95: 68.10% | 96: 68.10% | 97: 68.10% | 98: 68.10% | 99: 68.30% | 100: 68.30%\n","Cluster Recall -> 2: 75.00% | 10: 42.24%\n","MRR -> optimistic: 95.29% | pessimistic: 11.75%\n"]}]},{"cell_type":"markdown","source":["## Class and Functions"],"metadata":{"id":"fEy-ALy0P8r6"}},{"cell_type":"code","source":["class Mapper(torch.nn.Module):\n","\n","    def __init__(self) -> None:\n","        super(Mapper, self).__init__()\n","        self.linear = torch.nn.Linear(768, 768)\n","        self.reset()\n","\n","    def reset(self) -> None:\n","        with torch.no_grad():\n","            self.linear.weight.data = torch.eye(768).to(device=DEVICE)\n","            self.linear.bias.zero_()\n","\n","    def forward(self, batch_query_embeddings:torch.Tensor) -> torch.Tensor:\n","        batch_mapped_query_embeddings = self.linear(batch_query_embeddings)\n","        return batch_mapped_query_embeddings\n","\n","def get_positive_indices(dataset:Dataset, query_index:int, query_baseline_retrieved_passage_indices:np.array, total:int, mode:str) -> list[int]:\n","    if mode == 'random':\n","        positive_index_list = random.sample(list(dataset.relation_list[query_index]), total)\n","    else:\n","        if mode == 'worst-worst' or mode == 'worst-best':\n","            query_baseline_retrieved_passage_indices = np.flipud(query_baseline_retrieved_passage_indices)\n","        positive_index_list = list[int]()\n","        for passage_index in query_baseline_retrieved_passage_indices:\n","            if len(positive_index_list) == total:\n","                break\n","            if passage_index in dataset.relation_list[query_index]:\n","                positive_index_list.append(passage_index)\n","    return positive_index_list\n","\n","def get_negative_indices(dataset:Dataset, query_index:int, query_baseline_retrieved_passage_indices:np.array, total:int, mode:str) -> list[int]:\n","    if mode == 'random':\n","        negative_index_list = list[int]()\n","        while len(negative_index_list) < total:\n","            negative_index = random.choice(list(dataset.train_set))\n","            while negative_index in dataset.relation_list[query_index]:\n","                negative_index = random.choice(list(dataset.train_set))\n","            negative_index_list.append(negative_index)\n","    else:\n","        if mode == 'worst-worst' or mode == 'best-worst':\n","            query_baseline_retrieved_passage_indices = np.flipud(query_baseline_retrieved_passage_indices)\n","        negative_index_list = list[int]()\n","        for passage_index in query_baseline_retrieved_passage_indices:\n","            if len(negative_index_list) == total:\n","                break\n","            if passage_index not in dataset.relation_list[query_index]:\n","                negative_index_list.append(passage_index)\n","    return negative_index_list\n","\n","def get_targets(dataset:Dataset, passage_embeddings:np.array, baseline_retrieved_passage_indices:np.array, batch_query_index_list:list[int], preferred_total:int, positive_tendency:float, mode:str) -> tuple[torch.Tensor, torch.Tensor]:\n","    batch_total_positives_list = list[int]()\n","    batch_total_negatives_list = list[int]()\n","    for query_index in batch_query_index_list:\n","        total_positives = preferred_total * positive_tendency\n","        total_negatives = preferred_total - total_positives\n","        total_positives_error = max(1.0, total_positives / len(dataset.relation_list[query_index]))\n","        total_positives = round(total_positives / total_positives_error)\n","        total_negatives = round(total_negatives / total_positives_error)\n","        batch_total_positives_list.append(total_positives)\n","        batch_total_negatives_list.append(total_negatives)\n","    batch_positive_embeddings = torch.full((len(batch_query_index_list), max(batch_total_positives_list), 768), float('nan'), device=DEVICE)\n","    batch_negative_embeddings = torch.full((len(batch_query_index_list), max(batch_total_negatives_list), 768), float('nan'), device=DEVICE)\n","    for i, (query_index, total_positives, total_negatives) in enumerate(zip(batch_query_index_list, batch_total_positives_list, batch_total_negatives_list)):\n","        positive_index_list = get_positive_indices(dataset, query_index, baseline_retrieved_passage_indices[query_index, :], total_positives, mode)\n","        negative_index_list = get_negative_indices(dataset, query_index, baseline_retrieved_passage_indices[query_index, :], total_negatives, mode)\n","        batch_positive_embeddings[i, :len(positive_index_list), :] = torch.from_numpy(passage_embeddings[positive_index_list, :]).to(device=DEVICE)\n","        batch_negative_embeddings[i, :len(negative_index_list), :] = torch.from_numpy(passage_embeddings[negative_index_list, :]).to(device=DEVICE)\n","    return batch_positive_embeddings, batch_negative_embeddings\n","\n","def get_loss(batch_mapped_query_embeddings:torch.Tensor, batch_positive_embeddings:torch.Tensor, batch_negative_embeddings:torch.Tensor, margin:float, norm_order:int) -> torch.Tensor:\n","    batch_aggregated_positive_embeddings = torch.nanmean(batch_positive_embeddings, dim=1)\n","    batch_aggregated_negative_embeddings = torch.nanmean(batch_negative_embeddings, dim=1)\n","    batch_positive_scores = torch.norm(batch_mapped_query_embeddings - batch_aggregated_positive_embeddings, p=norm_order, dim=1)\n","    batch_negative_scores = torch.norm(batch_mapped_query_embeddings - batch_aggregated_negative_embeddings, p=norm_order, dim=1)\n","    positive_loss = torch.nanmean(torch.abs(batch_positive_scores)**2)\n","    negative_loss = torch.nanmean(torch.relu(margin - batch_negative_scores)**2)\n","    loss = None\n","    if not torch.isnan(positive_loss) and not torch.isnan(negative_loss):\n","        loss = positive_loss + negative_loss\n","    elif not torch.isnan(positive_loss):\n","        loss = positive_loss\n","    elif not torch.isnan(negative_loss):\n","        loss = negative_loss\n","    return loss"],"metadata":{"id":"58apNqV5ispT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Development"],"metadata":{"id":"pc5Lh8iPQnK7"}},{"cell_type":"code","source":["total_epochs = 50\n","patience = 3\n","\n","batch_size = 512\n","learning_rate = 0.0006598\n","preferred_total = 2\n","positive_tendency = 0.75\n","mode = 'worst-worst'\n","margin = 0.2068\n","norm_order = 3\n","\n","mapper = Mapper().to(device=DEVICE)\n","optimizer = torch.optim.Adam(mapper.parameters(), lr=learning_rate)\n","\n","best_mapper = mapper\n","best_validation_avg_pessimistic_mrr = -float('inf')\n","total_epochs_since_improvement = 0\n","for epoch in range(total_epochs):\n","\n","    with tqdm(total=len(ms_marco_dataset.train_set) // batch_size, desc=f'Epoch {epoch + 1:02}/{total_epochs}') as pbar:\n","\n","        mapper.train()\n","        loss_list = list[float]()\n","        for step in range(len(ms_marco_dataset.train_set) // batch_size):\n","\n","            batch_query_index_list = random.sample(list(ms_marco_dataset.train_set), batch_size)\n","            batch_query_embeddings = torch.from_numpy(ms_marco_b3_query_embeddings[batch_query_index_list, :]).to(device=DEVICE)\n","            batch_mapped_query_embeddings = mapper(batch_query_embeddings)\n","\n","            batch_positive_embeddings, batch_negative_embeddings = get_targets(ms_marco_dataset, ms_marco_b3_passage_embeddings, ms_marco_b3_retrieved_passage_indices, batch_query_index_list, preferred_total, positive_tendency, mode)\n","            loss = get_loss(batch_mapped_query_embeddings, batch_positive_embeddings, batch_negative_embeddings, margin, norm_order)\n","            if loss is not None:\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","                loss_list.append(loss.item())\n","\n","            pbar.set_postfix_str(f'loss: {np.mean(loss_list):.4f}', refresh=False)\n","            pbar.update(1)\n","\n","        avg_loss = np.mean(loss_list)\n","\n","        mapper.eval()\n","        with torch.no_grad():\n","\n","            sample_train_query_index_list = random.sample(list(ms_marco_dataset.train_set), batch_size)\n","            sample_train_query_embeddings = torch.from_numpy(ms_marco_b3_query_embeddings[sample_train_query_index_list, :]).to(device=DEVICE)\n","            sample_train_mapped_query_embeddings = mapper(sample_train_query_embeddings)\n","            sample_train_retrieved_passage_indices = ms_marco_semantic_searcher.search(sample_train_mapped_query_embeddings.cpu().numpy())\n","            _, _, avg_train_optimistic_mrr, avg_train_pessimistic_mrr = ms_marco_dataset.get_metrics(sample_train_query_index_list, sample_train_retrieved_passage_indices)\n","\n","            validation_query_index_list = list(ms_marco_dataset.validation_set)\n","            validation_query_embeddings = torch.from_numpy(ms_marco_b3_query_embeddings[validation_query_index_list, :]).to(device=DEVICE)\n","            validation_mapped_query_embeddings = mapper(validation_query_embeddings)\n","            validation_retrieved_passage_indices = ms_marco_semantic_searcher.search(validation_mapped_query_embeddings.cpu().numpy())\n","            _, _, avg_validation_optimistic_mrr, avg_validation_pessimistic_mrr = ms_marco_dataset.get_metrics(validation_query_index_list, validation_retrieved_passage_indices)\n","\n","            pbar.set_postfix_str(f'train (loss: {avg_loss:.4f}, o-mrr: {avg_train_optimistic_mrr:.4f}, p-mrr: {avg_train_pessimistic_mrr:.4f}), validation (o-mrr: {avg_validation_optimistic_mrr:.4f}, p-mrr: {avg_validation_pessimistic_mrr:.4f})', refresh=True)\n","\n","    if avg_validation_pessimistic_mrr > best_validation_avg_pessimistic_mrr:\n","        best_mapper = copy.deepcopy(mapper)\n","        best_validation_avg_pessimistic_mrr = avg_validation_pessimistic_mrr\n","        total_epochs_since_improvement = 0\n","    else:\n","        total_epochs_since_improvement += 1\n","    if total_epochs_since_improvement >= patience:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lfBF4CMeKN76","executionInfo":{"status":"ok","timestamp":1737680989457,"user_tz":-60,"elapsed":4172,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}},"outputId":"314204a9-e440-4e19-f058-f45536e2d104"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 01/50: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s, train (loss: 0.1150, o-mrr: 0.9860, p-mrr: 0.7948), validation (o-mrr: 1.0000, p-mrr: 0.8023)]\n","Epoch 02/50: 100%|██████████| 1/1 [00:00<00:00,  1.78it/s, train (loss: 0.1064, o-mrr: 0.9872, p-mrr: 0.8114), validation (o-mrr: 1.0000, p-mrr: 0.8083)]\n","Epoch 03/50: 100%|██████████| 1/1 [00:00<00:00,  1.95it/s, train (loss: 0.1000, o-mrr: 0.9873, p-mrr: 0.8287), validation (o-mrr: 1.0000, p-mrr: 0.8142)]\n","Epoch 04/50: 100%|██████████| 1/1 [00:00<00:00,  1.99it/s, train (loss: 0.0946, o-mrr: 0.9875, p-mrr: 0.8391), validation (o-mrr: 1.0000, p-mrr: 0.8157)]\n","Epoch 05/50: 100%|██████████| 1/1 [00:00<00:00,  1.99it/s, train (loss: 0.0903, o-mrr: 0.9882, p-mrr: 0.8514), validation (o-mrr: 1.0000, p-mrr: 0.8147)]\n","Epoch 06/50: 100%|██████████| 1/1 [00:00<00:00,  1.96it/s, train (loss: 0.0864, o-mrr: 0.9891, p-mrr: 0.8542), validation (o-mrr: 1.0000, p-mrr: 0.8146)]\n","Epoch 07/50: 100%|██████████| 1/1 [00:00<00:00,  1.99it/s, train (loss: 0.0836, o-mrr: 0.9888, p-mrr: 0.8649), validation (o-mrr: 1.0000, p-mrr: 0.8134)]"]},{"output_type":"stream","name":"stdout","text":["\n","test (o-mrr: 0.9883 | p-mrr: 0.8359)\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["## Evaluation"],"metadata":{"id":"BAAm4u-mQpvj"}},{"cell_type":"code","source":["mapper.eval()\n","with torch.no_grad():\n","    test_query_index_list = list(ms_marco_dataset.test_set)\n","    test_query_embeddings = torch.from_numpy(ms_marco_b3_query_embeddings[test_query_index_list, :]).to(device=DEVICE)\n","    test_mapped_query_embeddings = mapper(test_query_embeddings)\n","    test_retrieved_passage_indices = ms_marco_semantic_searcher.search(test_mapped_query_embeddings.cpu().numpy())\n","    print('Proposed Method: Semantic Search (MS-Marco, Test)')\n","    ms_marco_dataset.print_metrics(test_query_index_list, test_retrieved_passage_indices)"],"metadata":{"id":"QFVAMHI_QYPV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mapper.eval()\n","with torch.no_grad():\n","    test_query_index_list = list(hotpot_qa_dataset.test_set)\n","    test_query_embeddings = torch.from_numpy(hotpot_qa_b3_query_embeddings[test_query_index_list, :]).to(device=DEVICE)\n","    test_mapped_query_embeddings = mapper(test_query_embeddings)\n","    test_retrieved_passage_indices = hotpot_qa_semantic_searcher.search(test_mapped_query_embeddings.cpu().numpy())\n","    print('Proposed Method: Semantic Search (Hotpot-QA, Test)')\n","    hotpot_qa_dataset.print_metrics(test_query_index_list, test_retrieved_passage_indices)"],"metadata":{"id":"e8HNVZjrQh8D"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["rq2c9LClTEPh","XFd-W_zwTIUV","dUZQ4D2K79Bb","sHyt5bXp8AYv","B9ja7XCi8HUx","SDuRJDRl8Rym","4m9pIpKE8LW3","90AuD6qXHUTC"],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}