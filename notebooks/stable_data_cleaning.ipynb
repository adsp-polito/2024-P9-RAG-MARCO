{"cells":[{"cell_type":"markdown","metadata":{"id":"rq2c9LClTEPh"},"source":["# Notebook Initialization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_j14DIQ62Ph0"},"outputs":[],"source":["!pip install datasets\n","!pip install groq\n","!pip install spacy\n","!python -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Rq-rgwZlMXT"},"outputs":[],"source":["from typing import Iterable, Any\n","from google.colab import drive\n","from tqdm import tqdm\n","from groq import Groq\n","import os\n","import re\n","import pickle\n","import json\n","import pandas as pd\n","import numpy as np\n","import datasets\n","import spacy"]},{"cell_type":"code","source":["drive.mount('/content/drive')\n","nlp = spacy.load('en_core_web_sm')\n","groq_client = Groq(api_key='YOUR API KEY FROM GROQ')\n","\n","DATASET_ROOT = '/content/drive/MyDrive/ADSP Project/datasets/'\n","SPLIT_RATIOS = (0.8, 0.1, 0.1)\n","GROQ_MODEL = 'llama3-8b-8192'\n","\n","if not os.path.exists(DATASET_ROOT):\n","    raise ValueError('Invalid data root')\n","if any(split_ratio <= 0 for split_ratio in SPLIT_RATIOS) or sum(SPLIT_RATIOS) != 1.0:\n","    raise ValueError('Invalid split ratio')\n","groq_client.models.retrieve(GROQ_MODEL)"],"metadata":{"id":"mUQCIfmZtACK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset Class"],"metadata":{"id":"AC_3ltzor2Oy"}},{"cell_type":"code","source":["class Dataset:\n","\n","    def __init__(self, file_name:str, dataset_name:str=None) -> None:\n","        self._file_name:str = file_name\n","        self._stat_dict = {\n","            'passages': dict[str, int](),\n","            'queries': dict[str, int](),\n","            'augmentations': dict[str, int](),\n","            'relations': dict[str, int](),\n","            'learning': dict[str, int]()\n","        }\n","        self.dataset_name = dataset_name\n","        self.passage_list = list[str]()\n","        self.query_list = list[str]()\n","        self.passage_augmentation_list = list[dict[str, dict[str, int]]]()\n","        self.query_augmentation_list = list[dict[str, dict[str, int]]]()\n","        self.augmentation_dict = dict[str, set[int]]()\n","        self.relation_list = list[set[int]]()\n","        self.train_set = set[int]()\n","        self.validation_set = set[int]()\n","        self.test_set = set[int]()\n","        potential_dataset_path = os.path.join(DATASET_ROOT, f'{file_name}.pickle')\n","        if os.path.exists(potential_dataset_path):\n","            with open(potential_dataset_path, 'rb') as file_handle:\n","                public_dataset = pickle.load(file_handle)\n","                for attribute in public_dataset:\n","                    setattr(self, attribute, public_dataset[attribute])\n","        elif dataset_name is None:\n","            raise ValueError('Invalid file name')\n","        elif dataset_name not in {'ms-marco', 'hotpot-qa'}:\n","            raise ValueError('Invalid dataset name')\n","        self._update_stat()\n","\n","    def __str__(self) -> str:\n","        output = f'\\nnames -> file: {self._file_name}, dataset: {self.dataset_name}\\n'\n","        for stat in self._stat_dict:\n","            if len(self._stat_dict[stat]) == 0:\n","                continue\n","            output += f'{stat} -> ' + ', '.join(f'{attribute}: {self._stat_dict[stat][attribute]}' for attribute in self._stat_dict[stat]) + '\\n'\n","        return output\n","\n","    def _shuffle(self) -> None:\n","        def __shuffle_elemenet_list(element_list:list[Any], shuffle_map:list[int]) -> None:\n","            temp = element_list.copy()\n","            for i in range(len(temp)):\n","                element_list[shuffle_map[i]] = temp[i]\n","        def __shuffle_index_set(index_set:set[Any], shuffle_map:list[int]) -> None:\n","            temp = index_set.copy()\n","            index_set.clear()\n","            for index in temp:\n","                index_set.add(shuffle_map[index])\n","        passage_suffle_map = np.random.permutation(len(self.passage_list)).tolist()\n","        query_shuffle_map = np.random.permutation(len(self.query_list)).tolist()\n","        __shuffle_elemenet_list(self.passage_list, passage_suffle_map)\n","        __shuffle_elemenet_list(self.query_list, query_shuffle_map)\n","        __shuffle_elemenet_list(self.passage_augmentation_list, passage_suffle_map)\n","        __shuffle_elemenet_list(self.query_augmentation_list, query_shuffle_map)\n","        for augmentation_name in self.augmentation_dict:\n","            __shuffle_index_set(self.augmentation_dict[augmentation_name], query_shuffle_map)\n","        __shuffle_elemenet_list(self.relation_list, query_shuffle_map)\n","        for relation_set in self.relation_list:\n","            __shuffle_index_set(relation_set, passage_suffle_map)\n","        __shuffle_index_set(self.train_set, query_shuffle_map)\n","        __shuffle_index_set(self.validation_set, query_shuffle_map)\n","        __shuffle_index_set(self.test_set, query_shuffle_map)\n","\n","    def _update_stat(self) -> None:\n","        def __count_quantity(key:str, suffix:str, target_list:list[Any]) -> None:\n","            self._stat_dict[key][f'total_{suffix}'] = len(target_list)\n","        def __compute_stat(key:str, suffix:str, target_list:list[Iterable]) -> None:\n","            if len(target_list) > 0:\n","                self._stat_dict[key][f'minimum_{suffix}'] = min(len(iterable) for iterable in target_list)\n","                self._stat_dict[key][f'average_{suffix}'] = round(sum(len(iterable) for iterable in target_list) / len(target_list))\n","                self._stat_dict[key][f'maximum_{suffix}'] = max(len(iterable) for iterable in target_list)\n","        __count_quantity('passages', '', self.passage_list)\n","        __compute_stat('passages', 'length', self.passage_list)\n","        __count_quantity('queries', '', self.query_list)\n","        __compute_stat('queries', 'length', self.query_list)\n","        for augmentation_name in self.augmentation_dict:\n","            __count_quantity('augmentations', f'queries_augmented_with_{augmentation_name}', self.augmentation_dict[augmentation_name])\n","        __compute_stat('relations', 'related_passages', self.relation_list)\n","        __count_quantity('learning', 'queries_in_train_set', self.train_set)\n","        __count_quantity('learning', 'queries_in_validation_set', self.validation_set)\n","        __count_quantity('learning', 'queries_in_test_set', self.test_set)\n","\n","    def save(self) -> None:\n","        self._update_stat()\n","        dataset_path = os.path.join(DATASET_ROOT, f'{self._file_name}.pickle')\n","        with open(dataset_path, 'wb') as file_handle:\n","            public_dataset = {attribute: getattr(self, attribute) for attribute in self.__dict__ if not attribute.startswith('_')}\n","            pickle.dump(public_dataset, file_handle, protocol=pickle.HIGHEST_PROTOCOL)\n","        stat_path = os.path.join(DATASET_ROOT, 'stat.json')\n","        if os.path.exists(stat_path):\n","            with open(stat_path, 'r') as file_handle:\n","                full_stat = json.load(file_handle)\n","        else:\n","            full_stat = dict[str, dict[str, dict[str, int]]]()\n","        full_stat[self._file_name] = {'dataset_name': self.dataset_name} | self._stat_dict\n","        with open(stat_path, 'w') as file_handle:\n","            json.dump(full_stat, file_handle, indent=4)\n","\n","    def add_points(self, total_queries:int) -> None:\n","        def __expand_ms_marco_split(split_name:str, split_ratio:float) -> None:\n","            extra_split_size = int(total_queries * split_ratio)\n","            with tqdm(total=extra_split_size, desc=f'Downloading {split_name} set from {self.dataset_name} dataset') as pbar:\n","                stream = datasets.load_dataset('microsoft/ms_marco', 'v1.1', split=split_name, streaming=True)\n","                target_learning_set:set[int] = getattr(self, split_name + '_set')\n","                for point in stream.skip(len(target_learning_set)).take(extra_split_size):\n","                    current_passage_index = len(self.passage_list)\n","                    current_query_index = len(self.query_list)\n","                    extra_passage_list:list[str] = point['passages']['passage_text']\n","                    extra_query:str = point['query']\n","                    for passage in extra_passage_list:\n","                        self.passage_list.append(passage)\n","                        self.passage_augmentation_list.append(dict[str, dict[str, int]]())\n","                    self.query_list.append(extra_query)\n","                    self.query_augmentation_list.append(dict[str, dict[str, int]]())\n","                    self.relation_list.append(set(range(current_passage_index, len(self.passage_list))))\n","                    target_learning_set.add(current_query_index)\n","                    pbar.update(1)\n","        def __expand_hotpot_qa_split(split_name:str, split_ratio:float) -> None:\n","            extra_split_size = int(total_queries * split_ratio)\n","            with tqdm(total=extra_split_size, desc=f'Downloading {split_name} set from {self.dataset_name} dataset') as pbar:\n","                stream = datasets.load_dataset('hotpot_qa', 'fullwiki', split=split_name, streaming=True)\n","                target_learning_set:set[int] = getattr(self, split_name + '_set')\n","                for point in stream.skip(len(target_learning_set)).take(extra_split_size):\n","                    current_passage_index = len(self.passage_list)\n","                    current_query_index = len(self.query_list)\n","                    extra_passage_list = list[str]()\n","                    for index, title in enumerate(point['context'][\"title\"]):\n","                        document = [title]\n","                        for sentence in point['context']['sentences'][index]:\n","                            document.append(sentence)\n","                        extra_passage_list.append(\"\\n\".join(document))\n","                    extra_query:str = point['question']\n","                    for passage in extra_passage_list:\n","                        self.passage_list.append(passage)\n","                        self.passage_augmentation_list.append(dict[str, dict[str, int]]())\n","                    self.query_list.append(extra_query)\n","                    self.query_augmentation_list.append(dict[str, dict[str, int]]())\n","                    self.relation_list.append(set(range(current_passage_index, len(self.passage_list))))\n","                    target_learning_set.add(current_query_index)\n","                    pbar.update(1)\n","        if self.dataset_name == 'ms-marco':\n","            for split_name, split_ratio in zip(['train', 'validation', 'test'], SPLIT_RATIOS):\n","                __expand_ms_marco_split(split_name, split_ratio)\n","        elif self.dataset_name == 'hotpot-qa':\n","            for split_name, split_ratio in zip(['train', 'validation', 'test'], SPLIT_RATIOS):\n","                __expand_hotpot_qa_split(split_name, split_ratio)\n","        self._shuffle()\n","        self._update_stat()\n","\n","    def augment_with_ner(self, total_queries:int=None) -> None:\n","        def __initialize(augmentation_name:str) -> tuple[set[int], int]:\n","            if augmentation_name not in self.augmentation_dict:\n","                self.augmentation_dict[augmentation_name] = set[int]()\n","            chosen_query_index_set = set[int]()\n","            for query_index in np.random.permutation(len(self.query_list)).tolist():\n","                if query_index not in self.augmentation_dict[augmentation_name]:\n","                    chosen_query_index_set.add(query_index)\n","                if len(chosen_query_index_set) == total_queries:\n","                    break\n","            total_texts = sum(len(self.relation_list[query_index]) for query_index in chosen_query_index_set)\n","            return chosen_query_index_set, total_texts\n","        def __extract_ner(text:str) -> dict[str, dict[str, int]]:\n","            ner_dict = dict[str, dict[str, int]]()\n","            for ent in nlp(text).ents:\n","                key = 'spacy_entity_' + ent.label_.lower().strip()\n","                entity = ent.text.lower().strip()\n","                if key not in ner_dict:\n","                    ner_dict[key] = dict[str, int]()\n","                if entity not in ner_dict[key]:\n","                    ner_dict[key][entity] = 0\n","                ner_dict[key][entity] += 1\n","            return ner_dict\n","        chosen_query_index_set, total_texts = __initialize('spacy_ner')\n","        with tqdm(total=total_texts, desc='Augmenting with Spacy NER') as pbar:\n","            for query_index in chosen_query_index_set:\n","                query_ner_dict = __extract_ner(self.query_list[query_index])\n","                complex_passage_ner_dict = dict[int, dict[str, dict[str, int]]]()\n","                for passage_index in self.relation_list[query_index]:\n","                    complex_passage_ner_dict[passage_index] = __extract_ner(self.passage_list[passage_index])\n","                    pbar.update(1)\n","                for key in query_ner_dict:\n","                    self.query_augmentation_list[query_index][key] = query_ner_dict[key]\n","                for passage_index, passage_ner_dict in complex_passage_ner_dict.items():\n","                    for key in passage_ner_dict:\n","                        self.passage_augmentation_list[passage_index][key] = passage_ner_dict[key]\n","                self.augmentation_dict['spacy_ner'].add(query_index)\n","        self._update_stat()\n","\n","    def augment_with_keyword_and_topic(self, total_queries:int=None) -> None:\n","        def __initialize(augmentation_name:str) -> tuple[set[int], int]:\n","                if augmentation_name not in self.augmentation_dict:\n","                    self.augmentation_dict[augmentation_name] = set[int]()\n","                chosen_query_index_set = set[int]()\n","                for query_index in np.random.permutation(len(self.query_list)).tolist():\n","                    if query_index not in self.augmentation_dict[augmentation_name]:\n","                        chosen_query_index_set.add(query_index)\n","                    if len(chosen_query_index_set) == total_queries:\n","                        break\n","                total_texts = sum(len(self.relation_list[query_index]) for query_index in chosen_query_index_set)\n","                return chosen_query_index_set, total_texts\n","        def __extract_keywords_and_topics(text:str) -> tuple[set[str], set[str]]:\n","            chat_completion = groq_client.chat.completions.create(\n","                messages=[\n","                    {'role': 'system', 'content': (\n","                        'You are an AI assistant tasked with identifying keywords and topics from the given text.'\n","                        '\\nYour task rules are as follows:'\n","                        '\\n- The output is the following dictionary:'\n","                        '\\n  {'\n","                        '\\n      \"keyword_list\": [list of most important keywords],'\n","                        '\\n      \"topic_list\": [list of most important topics]'\n","                        '\\n- Your response must be the output dictionary in JSON format without any extra information.'\n","                    )},\n","                    {'role': 'user', 'content': (\n","                        'Here is the text:'\n","                        '\\n' + text + ''\n","                    )}\n","                ],\n","                model = GROQ_MODEL\n","            )\n","            response = chat_completion.choices[0].message.content\n","            keyword_set = set[str]()\n","            try:\n","                matched_groups = re.search(r'\\\"keyword_list\\\"\\s*:\\s*(\\[[^\\]]*\\])', response)\n","                for potential_keyword in set(json.loads(matched_groups.group(1))):\n","                    if not isinstance(potential_keyword, str):\n","                        raise BaseException()\n","                    potential_keyword = potential_keyword.lower().strip()\n","                    if len(potential_keyword) == 0:\n","                        raise BaseException()\n","                    keyword_set.add(potential_keyword)\n","            except BaseException:\n","                pass\n","            topic_set = set[str]()\n","            try:\n","                matched_groups = re.search(r'\\\"topic_list\\\"\\s*:\\s*(\\[[^\\]]*\\])', response)\n","                for potential_topic in set(json.loads(matched_groups.group(1))):\n","                    if not isinstance(potential_topic, str):\n","                        raise BaseException()\n","                    potential_topic = potential_topic.lower().strip()\n","                    if len(potential_topic) == 0:\n","                        raise BaseException()\n","                    topic_set.add(potential_topic)\n","            except BaseException:\n","                pass\n","            return keyword_set, topic_set\n","        chosen_query_index_set, total_texts = __initialize(f'{GROQ_MODEL}_keyword_and_topic_extraction')\n","        with tqdm(total=total_texts, desc=f'Augmenting with {GROQ_MODEL} Keyword and Topic Extraction') as pbar:\n","            for query_index in chosen_query_index_set:\n","                query_keyword_set, query_topic_set = __extract_keywords_and_topics(self.query_list[query_index])\n","                if len(query_keyword_set) == 0 or len(query_topic_set) == 0:\n","                    pbar.update(len(self.relation_list[query_index]))\n","                    continue\n","                passage_keyword_and_topic_dict = dict[int, tuple[set[str], set[str]]]()\n","                for passage_index in self.relation_list[query_index]:\n","                    passage_keyword_set, passage_topic_set = __extract_keywords_and_topics(self.passage_list[passage_index])\n","                    pbar.update(1)\n","                    if len(passage_keyword_set) == 0 or len(passage_topic_set) == 0:\n","                        break\n","                    passage_keyword_and_topic_dict[passage_index] = passage_keyword_set, passage_topic_set\n","                if len(self.relation_list[query_index]) - len(passage_keyword_and_topic_dict) != 0:\n","                    pbar.update(len(self.relation_list[query_index]) - len(passage_keyword_and_topic_dict))\n","                    continue\n","                self.query_augmentation_list[query_index][f'{GROQ_MODEL}_keyword'] = {keyword: 1 for keyword in query_keyword_set}\n","                self.query_augmentation_list[query_index][f'{GROQ_MODEL}_topic'] = {topic: 1 for topic in query_topic_set}\n","                for passage_index, (passage_keyword_set, passage_topic_set) in passage_keyword_and_topic_dict.items():\n","                    self.passage_augmentation_list[passage_index][f'{GROQ_MODEL}_keyword'] = dict.fromkeys(passage_keyword_set, 1)\n","                    self.passage_augmentation_list[passage_index][f'{GROQ_MODEL}_topic'] = dict.fromkeys(passage_topic_set, 1)\n","                self.augmentation_dict[f'{GROQ_MODEL}_keyword_and_topic_extraction'].add(query_index)\n","        self._update_stat()"],"metadata":{"id":"fAuxriDhmV7n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Operation"],"metadata":{"id":"aY8JLahqhJ6C"}},{"cell_type":"markdown","source":["## MS-MARCO"],"metadata":{"id":"F2_dQtSJq-uz"}},{"cell_type":"markdown","source":["### With Augmentation"],"metadata":{"id":"ZJ26iPyxb6jM"}},{"cell_type":"code","source":["ms_marco_dataset = Dataset('ms-marco-spacy-llama8b', 'ms-marco')\n","print(ms_marco_dataset)"],"metadata":{"id":"TJCf7HX0t93S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ms_marco_dataset.add_points(100)\n","ms_marco_dataset.save()\n","print(ms_marco_dataset)"],"metadata":{"id":"hkvy3gfRuLbs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ms_marco_dataset.augment_with_ner()\n","ms_marco_dataset.save()\n","print(ms_marco_dataset)"],"metadata":{"id":"F5XSWrzZwKG8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ms_marco_dataset.augment_with_keyword_and_topic(10)\n","ms_marco_dataset.save()\n","print(ms_marco_dataset)"],"metadata":{"id":"4-idg23CwNzc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Without Augmentation"],"metadata":{"id":"_cBjZ50-cCtW"}},{"cell_type":"code","source":["ms_marco_dataset = Dataset('ms-marco-no-augmentation', 'ms-marco')\n","print(ms_marco_dataset)"],"metadata":{"id":"L2jDfhc0cFq0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ms_marco_dataset.add_points(1000)\n","ms_marco_dataset.save()\n","print(ms_marco_dataset)"],"metadata":{"id":"MfhajrTSNMS0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Hotpot QA"],"metadata":{"id":"vMJpYeV6JI7x"}},{"cell_type":"markdown","source":["### With Augmentation"],"metadata":{"id":"jyCW1hv0ND9a"}},{"cell_type":"code","source":["hotpot_qa_dataset = Dataset('hotpot-qa-spacy-llama8b', 'hotpot-qa')\n","print(hotpot_qa_dataset)"],"metadata":{"id":"4mAl1gefNFjN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hotpot_qa_dataset.add_points(100)\n","hotpot_qa_dataset.save()\n","print(hotpot_qa_dataset)"],"metadata":{"id":"nMvY6dzCcQ78"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hotpot_qa_dataset.augment_with_ner()\n","hotpot_qa_dataset.save()\n","print(hotpot_qa_dataset)"],"metadata":{"id":"lpwWLshUMsm1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hotpot_qa_dataset.augment_with_keyword_and_topic(5)\n","hotpot_qa_dataset.save()\n","print(hotpot_qa_dataset)"],"metadata":{"id":"GthLJbNHMo3p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Without Augmentation"],"metadata":{"id":"-Spwj-fjNPRC"}},{"cell_type":"code","source":["hotpot_qa_dataset = Dataset('hotpot-qa-no-augmentation', 'hotpot-qa')\n","print(hotpot_qa_dataset)"],"metadata":{"id":"PXWn61w_NPxK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hotpot_qa_dataset.add_points(1000)\n","hotpot_qa_dataset.save()\n","print(hotpot_qa_dataset)"],"metadata":{"id":"8DN13gEZNTra"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["AC_3ltzor2Oy"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}