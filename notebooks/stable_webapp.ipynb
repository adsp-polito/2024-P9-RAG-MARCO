{"cells":[{"cell_type":"markdown","metadata":{"id":"rq2c9LClTEPh"},"source":["# Notebook Initialization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_j14DIQ62Ph0"},"outputs":[],"source":["!pip install datasets\n","!pip install sentence_transformers\n","!pip install faiss-cpu\n","!pip install faiss-gpu\n","!pip install scann\n","!pip install fastapi\n","!pip install python-multipart\n","!pip install pyngrok\n","!pip install uvicorn\n","!pip install groq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Rq-rgwZlMXT"},"outputs":[],"source":["from google.colab import drive\n","from typing import Any, Callable, Iterable\n","from sentence_transformers import SentenceTransformer\n","from tqdm import tqdm\n","from pyngrok import ngrok\n","from fastapi.middleware.cors import CORSMiddleware\n","from groq import Groq\n","import os\n","import time\n","import re\n","import json\n","import copy\n","import pickle\n","import enum\n","import torch\n","import faiss\n","import scann\n","import random\n","import numpy as np\n","import pandas as pd\n","import fastapi\n","import pyngrok\n","import asyncio\n","import uvicorn\n","import nest_asyncio"]},{"cell_type":"code","source":["drive.mount('/content/drive')\n","\n","DATASET_ROOT = '/content/drive/MyDrive/ADSP Project/datasets/'\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","RETRIEVAL_CAPACITY = 50\n","GROQ_MODEL = 'llama3-8b-8192'\n","GROQ_API_KEY = 'YOUR API KEY FROM GROQ'\n","NGROK_AUTHENTICATION_TOKEN = 'YOUR AUTHENTICATION TOKEN FROM NGROK'\n","INTERNAL_PORT = 8002\n","\n","if not os.path.exists(DATASET_ROOT):\n","    raise ValueError('Invalid data root')"],"metadata":{"id":"mhD5hshbLqsP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XFd-W_zwTIUV"},"source":["# Classes"]},{"cell_type":"markdown","source":["## Config"],"metadata":{"id":"dUZQ4D2K79Bb"}},{"cell_type":"code","source":["class Config:\n","\n","    class DATASET_NAMES(enum.Enum):\n","        MS_MARCO = 'ms-marco'\n","        HOTPOT_QA = 'hotpot-qa'\n","\n","    class TRANSFORMER_MODEL_NAMES(enum.Enum):\n","        ALL_MPNET_BASE_V2 = 'all-mpnet-base-v2'\n","        MULTI_QA_MPNET_BASE_DOT_V1 = 'multi-qa-mpnet-base-dot-v1'\n","        ALL_DISTILROBERTA_V1 = 'all-distilroberta-v1'\n","\n","    class VECTOR_DB_NAMES(enum.Enum):\n","        FAISS = 'faiss'\n","        SCANN = 'scann'\n","\n","    class SIMILARITY_METRIC_NAMES(enum.Enum):\n","        L2 = 'l2'\n","        IP = 'ip'\n","        CS = 'cs'"],"metadata":{"id":"G3LoTJBl7_kd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset"],"metadata":{"id":"sHyt5bXp8AYv"}},{"cell_type":"code","source":["class Dataset:\n","\n","    def __init__(self, file_name:str) -> None:\n","        self._file_name:str = file_name\n","        self._stat_dict = {\n","            'passages': dict[str, int](),\n","            'queries': dict[str, int](),\n","            'augmentations': dict[str, int](),\n","            'relations': dict[str, int](),\n","            'learning': dict[str, int]()\n","        }\n","        self.dataset_name:Config.DATASET_NAMES = None\n","        self.passage_list = list[str]()\n","        self.query_list = list[str]()\n","        self.passage_augmentation_list = list[dict[str, dict[str, int]]]()\n","        self.query_augmentation_list = list[dict[str, dict[str, int]]]()\n","        self.augmentation_dict = dict[str, set[int]]()\n","        self.relation_list = list[set[int]]()\n","        self.train_set = set[int]()\n","        self.validation_set = set[int]()\n","        self.test_set = set[int]()\n","        potential_dataset_path = os.path.join(DATASET_ROOT, f'{file_name}.pickle')\n","        if os.path.exists(potential_dataset_path):\n","            with open(potential_dataset_path, 'rb') as file_handle:\n","                public_dataset = pickle.load(file_handle)\n","                for attribute in public_dataset:\n","                    setattr(self, attribute, public_dataset[attribute])\n","            if self.dataset_name not in {item.value for item in Config.DATASET_NAMES}:\n","                raise ValueError('Invalid dataset name')\n","            self.dataset_name = Config.DATASET_NAMES(self.dataset_name)\n","        else:\n","            raise ValueError('Invalid file name')\n","        self._update_stat()\n","\n","    def __str__(self) -> str:\n","        output_list = [f'names -> file: {self._file_name}, dataset: {self.dataset_name}']\n","        for stat in self._stat_dict:\n","            if len(self._stat_dict[stat]) == 0:\n","                continue\n","            output_list.append(f'{stat} -> ' + ', '.join(f'{attribute}: {self._stat_dict[stat][attribute]}' for attribute in self._stat_dict[stat]))\n","        return '\\n'.join(output_list)\n","\n","    def _update_stat(self) -> None:\n","        def __count_quantity(key:str, suffix:str, target_list:list[Any]) -> None:\n","            self._stat_dict[key][f'total_{suffix}'] = len(target_list)\n","        def __compute_stat(key:str, suffix:str, target_list:list[Iterable]) -> None:\n","            if len(target_list) > 0:\n","                self._stat_dict[key][f'minimum_{suffix}'] = min(len(iterable) for iterable in target_list)\n","                self._stat_dict[key][f'average_{suffix}'] = round(sum(len(iterable) for iterable in target_list) / len(target_list))\n","                self._stat_dict[key][f'maximum_{suffix}'] = max(len(iterable) for iterable in target_list)\n","        __count_quantity('passages', '', self.passage_list)\n","        __compute_stat('passages', 'length', self.passage_list)\n","        __count_quantity('queries', '', self.query_list)\n","        __compute_stat('queries', 'length', self.query_list)\n","        for augmentation_name in self.augmentation_dict:\n","            __count_quantity('augmentations', f'queries_augmented_with_{augmentation_name}', self.augmentation_dict[augmentation_name])\n","        __compute_stat('relations', 'related_passages', self.relation_list)\n","        __count_quantity('learning', 'queries_in_train_set', self.train_set)\n","        __count_quantity('learning', 'queries_in_validation_set', self.validation_set)\n","        __count_quantity('learning', 'queries_in_test_set', self.test_set)\n","\n","    def _get_recall(self, query_index:int, query_retrieved_passage_indices:np.ndarray) -> list[float]:\n","        total_related_passages = len(self.relation_list[query_index])\n","        recall_list = list[float]()\n","        for k in range(1, query_retrieved_passage_indices.size + 1):\n","            recall_list.append(len(self.relation_list[query_index].intersection(query_retrieved_passage_indices[:k])) / total_related_passages)\n","        return recall_list\n","\n","    def _get_optimistic_mrr(self, query_index:int, query_retrieved_passage_indices:np.ndarray) -> float:\n","        optimistic_mrr = 0.0\n","        for rank, retrieved_passage_index in enumerate(query_retrieved_passage_indices, start=1):\n","            if retrieved_passage_index in self.relation_list[query_index]:\n","                optimistic_mrr = 1.0 / rank\n","                break\n","        return optimistic_mrr\n","\n","    def _get_pessimistic_mrr(self, query_index:int, query_retrieved_passage_indices:np.ndarray) -> float:\n","        total_related_passages = len(self.relation_list[query_index])\n","        pessimistic_mrr = 0.0\n","        for rank in range(total_related_passages, query_retrieved_passage_indices.size + 1):\n","            if len(self.relation_list[query_index].intersection(query_retrieved_passage_indices[:rank])) == total_related_passages:\n","                pessimistic_mrr = total_related_passages / rank\n","                break\n","        return pessimistic_mrr\n","\n","    def get_metrics(self, query_index_list:list[int], retrieved_passage_indices:np.ndarray) -> tuple[dict[int, float], dict[int, float], float, float]:\n","        recall_dict = dict[int, list[float]]()\n","        recall_star_dict = dict[int, list[float]]()\n","        optimistic_mrr_list = list[float]()\n","        pessimistic_mrr_list = list[float]()\n","        for i in range(len(query_index_list)):\n","            total_related_passages = len(self.relation_list[query_index_list[i]])\n","            if total_related_passages == 0:\n","                continue\n","            recall_list = self._get_recall(query_index_list[i], retrieved_passage_indices[i, :])\n","            optimistic_mrr = self._get_optimistic_mrr(query_index_list[i], retrieved_passage_indices[i, :])\n","            pessimistic_mrr = self._get_pessimistic_mrr(query_index_list[i], retrieved_passage_indices[i, :])\n","            for k, recall in enumerate(recall_list, start=1):\n","                if k not in recall_dict:\n","                    recall_dict[k] = list[float]()\n","                recall_dict[k].append(recall)\n","                if k == total_related_passages:\n","                    if total_related_passages not in recall_star_dict:\n","                        recall_star_dict[total_related_passages] = list[float]()\n","                    recall_star_dict[total_related_passages].append(recall)\n","            optimistic_mrr_list.append(optimistic_mrr)\n","            pessimistic_mrr_list.append(pessimistic_mrr)\n","        avg_recall_dict = {k: sum(recall_list) / len(recall_list) for k, recall_list in dict(sorted(recall_dict.items())).items()}\n","        avg_recall_start_dict = {total_related_passages: sum(recall_star_list) / len(recall_star_list) for total_related_passages, recall_star_list in dict(sorted(recall_star_dict.items())).items()}\n","        avg_optimistic_mrr = sum(optimistic_mrr_list) / len(optimistic_mrr_list)\n","        avg_pessimistic_mrr = sum(pessimistic_mrr_list) / len(pessimistic_mrr_list)\n","        return avg_recall_dict, avg_recall_start_dict, avg_optimistic_mrr, avg_pessimistic_mrr\n","\n","    def print_metrics(self, query_index_list:list[int], retrieved_passage_indices:np.ndarray) -> None:\n","        avg_recall_dict, avg_recall_start_dict, avg_optimistic_mrr, avg_pessimistic_mrr = self.get_metrics(query_index_list, retrieved_passage_indices)\n","        print('Dataset Name ->', self.dataset_name)\n","        print('Recall ->', ' | '.join(f'{k}: {100 * avg_recall:.2f}%' for k, avg_recall in avg_recall_dict.items()))\n","        print('Cluster Recall ->', ' | '.join(f'{total_related_passages}: {100 * avg_recall:.2f}%' for total_related_passages, avg_recall in avg_recall_start_dict.items()))\n","        print('MRR ->', f'optimistic: {100 * avg_optimistic_mrr:.2f}% | pessimistic: {100 * avg_pessimistic_mrr:.2f}%')"],"metadata":{"id":"_8OO2NbUsGR4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Transformer"],"metadata":{"id":"B9ja7XCi8HUx"}},{"cell_type":"code","source":["class Transformer:\n","\n","    def __init__(self, model_name:Config.TRANSFORMER_MODEL_NAMES) -> None:\n","        self.model_name = model_name\n","        self._transformer = SentenceTransformer(model_name_or_path=model_name.value, device=DEVICE)\n","        self._transformer.encode(['warm_up'])\n","\n","    def embed(self, text_list:list[str]) -> np.ndarray:\n","        print('Embedding ...', end='')\n","        embeddings = self._transformer.encode(text_list, convert_to_numpy=True)\n","        print(' done')\n","        return embeddings"],"metadata":{"id":"bJfXaXjV8Ipf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Semantic Searcher"],"metadata":{"id":"SDuRJDRl8Rym"}},{"cell_type":"code","source":["class SemanticSearcher:\n","\n","    def __init__(self, vectordb_name:Config.VECTOR_DB_NAMES, similarity_metric_name:Config.SIMILARITY_METRIC_NAMES) -> None:\n","        self.vectordb_name = vectordb_name\n","        self.similarity_metric_name = similarity_metric_name\n","        if self.vectordb_name == Config.VECTOR_DB_NAMES.FAISS:\n","            if self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.L2:\n","                self._engine = faiss.IndexFlatL2(768)\n","            elif self.similarity_metric_name in [Config.SIMILARITY_METRIC_NAMES.IP, Config.SIMILARITY_METRIC_NAMES.CS]:\n","                self._engine = faiss.IndexFlatIP(768)\n","        elif self.vectordb_name == Config.VECTOR_DB_NAMES.SCANN:\n","            self._engine:Any = None\n","\n","    def index(self, passage_embeddings:np.ndarray) -> None:\n","        if self.vectordb_name == Config.VECTOR_DB_NAMES.FAISS:\n","            if self.similarity_metric_name in [Config.SIMILARITY_METRIC_NAMES.L2, Config.SIMILARITY_METRIC_NAMES.IP]:\n","                self._engine.add(passage_embeddings)\n","            elif self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.CS:\n","                normalized_passage_embeddings = passage_embeddings / np.linalg.norm(passage_embeddings, axis=1, keepdims=True)\n","                self._engine.add(normalized_passage_embeddings)\n","        elif self.vectordb_name == Config.VECTOR_DB_NAMES.SCANN:\n","            if self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.L2:\n","                self._engine = scann.scann_ops_pybind.builder(passage_embeddings, RETRIEVAL_CAPACITY, 'squared_l2').score_brute_force().build()\n","            elif self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.IP:\n","                self._engine = scann.scann_ops_pybind.builder(passage_embeddings, RETRIEVAL_CAPACITY, 'dot_product').score_brute_force().build()\n","            elif self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.CS:\n","                normalized_passage_embeddings = passage_embeddings / np.linalg.norm(passage_embeddings, axis=1, keepdims=True)\n","                self._engine = scann.scann_ops_pybind.builder(normalized_passage_embeddings, RETRIEVAL_CAPACITY, 'dot_product').score_brute_force().build()\n","\n","    def search(self, query_embeddings:np.ndarray) -> np.ndarray:\n","        if self.vectordb_name == Config.VECTOR_DB_NAMES.FAISS:\n","            if self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.L2:\n","                retrieved_passage_indices = self._engine.search(query_embeddings, RETRIEVAL_CAPACITY)[1]\n","            elif self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.IP:\n","                retrieved_passage_indices = self._engine.search(query_embeddings, RETRIEVAL_CAPACITY)[1]\n","            elif self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.CS:\n","                normalized_query_embeddings = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n","                retrieved_passage_indices = self._engine.search(normalized_query_embeddings, RETRIEVAL_CAPACITY)[1]\n","        elif self.vectordb_name == Config.VECTOR_DB_NAMES.SCANN:\n","            retrieved_passage_index_matrix = list[list[int]]()\n","            if self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.L2:\n","                for i in range(query_embeddings.shape[0]):\n","                    retrieved_passage_index_list = self._engine.search(query_embeddings[i, :])[0]\n","                    retrieved_passage_index_matrix.append(retrieved_passage_index_list)\n","            elif self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.IP:\n","                for i in range(query_embeddings.shape[0]):\n","                    retrieved_passage_index_list = self._engine.search(query_embeddings[i, :])[0]\n","                    retrieved_passage_index_matrix.append(retrieved_passage_index_list)\n","            elif self.similarity_metric_name == Config.SIMILARITY_METRIC_NAMES.CS:\n","                normalized_query_embeddings = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n","                for i in range(normalized_query_embeddings.shape[0]):\n","                    retrieved_passage_index_list = self._engine.search(normalized_query_embeddings[i, :])[0]\n","                    retrieved_passage_index_matrix.append(retrieved_passage_index_list)\n","            retrieved_passage_indices = np.array(retrieved_passage_index_matrix)\n","        return retrieved_passage_indices"],"metadata":{"id":"x2VRsBtT8Q63"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aHWYgScvAKuE"},"source":["# Web Server"]},{"cell_type":"markdown","source":["## Initialization"],"metadata":{"id":"I6-WSuvnegif"}},{"cell_type":"code","source":["ms_marco_transformer = Transformer(Config.TRANSFORMER_MODEL_NAMES.ALL_MPNET_BASE_V2)\n","ms_marco_semantic_searcher = SemanticSearcher(Config.VECTOR_DB_NAMES.FAISS, Config.SIMILARITY_METRIC_NAMES.CS)\n","\n","ms_marco_dataset = Dataset('ms-marco-no-augmentation')\n","print()\n","print(ms_marco_dataset)\n","\n","ms_marco_b3_passage_embeddings = ms_marco_transformer.embed(ms_marco_dataset.passage_list)\n","ms_marco_b3_query_embeddings = ms_marco_transformer.embed(ms_marco_dataset.query_list)\n","ms_marco_semantic_searcher.index(ms_marco_b3_passage_embeddings)\n","ms_marco_b3_retrieved_passage_indices = ms_marco_semantic_searcher.search(ms_marco_b3_query_embeddings)"],"metadata":{"id":"9ME2JMjLPy8B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Class and Functions"],"metadata":{"id":"fEy-ALy0P8r6"}},{"cell_type":"code","source":["class Mapper(torch.nn.Module):\n","\n","    def __init__(self) -> None:\n","        super(Mapper, self).__init__()\n","        self.linear = torch.nn.Linear(768, 768)\n","        self.reset()\n","\n","    def reset(self) -> None:\n","        with torch.no_grad():\n","            self.linear.weight.data = torch.eye(768).to(device=DEVICE)\n","            self.linear.bias.zero_()\n","\n","    def forward(self, batch_query_embeddings:torch.Tensor) -> torch.Tensor:\n","        batch_mapped_query_embeddings = self.linear(batch_query_embeddings)\n","        return batch_mapped_query_embeddings\n","\n","def get_positive_indices(dataset:Dataset, query_index:int, query_baseline_retrieved_passage_indices:np.array, total:int, mode:str) -> list[int]:\n","    if mode == 'random':\n","        positive_index_list = random.sample(list(dataset.relation_list[query_index]), total)\n","    else:\n","        if mode == 'worst-worst' or mode == 'worst-best':\n","            query_baseline_retrieved_passage_indices = np.flipud(query_baseline_retrieved_passage_indices)\n","        positive_index_list = list[int]()\n","        for passage_index in query_baseline_retrieved_passage_indices:\n","            if len(positive_index_list) == total:\n","                break\n","            if passage_index in dataset.relation_list[query_index]:\n","                positive_index_list.append(passage_index)\n","    return positive_index_list\n","\n","def get_negative_indices(dataset:Dataset, query_index:int, query_baseline_retrieved_passage_indices:np.array, total:int, mode:str) -> list[int]:\n","    if mode == 'random':\n","        negative_index_list = list[int]()\n","        while len(negative_index_list) < total:\n","            negative_index = random.choice(list(dataset.train_set))\n","            while negative_index in dataset.relation_list[query_index]:\n","                negative_index = random.choice(list(dataset.train_set))\n","            negative_index_list.append(negative_index)\n","    else:\n","        if mode == 'worst-worst' or mode == 'best-worst':\n","            query_baseline_retrieved_passage_indices = np.flipud(query_baseline_retrieved_passage_indices)\n","        negative_index_list = list[int]()\n","        for passage_index in query_baseline_retrieved_passage_indices:\n","            if len(negative_index_list) == total:\n","                break\n","            if passage_index not in dataset.relation_list[query_index]:\n","                negative_index_list.append(passage_index)\n","    return negative_index_list\n","\n","def get_targets(dataset:Dataset, passage_embeddings:np.array, baseline_retrieved_passage_indices:np.array, batch_query_index_list:list[int], preferred_total:int, positive_tendency:float, mode:str) -> tuple[torch.Tensor, torch.Tensor]:\n","    batch_total_positives_list = list[int]()\n","    batch_total_negatives_list = list[int]()\n","    for query_index in batch_query_index_list:\n","        total_positives = preferred_total * positive_tendency\n","        total_negatives = preferred_total - total_positives\n","        total_positives_error = max(1.0, total_positives / len(dataset.relation_list[query_index]))\n","        total_positives = round(total_positives / total_positives_error)\n","        total_negatives = round(total_negatives / total_positives_error)\n","        batch_total_positives_list.append(total_positives)\n","        batch_total_negatives_list.append(total_negatives)\n","    batch_positive_embeddings = torch.full((len(batch_query_index_list), max(batch_total_positives_list), 768), float('nan'), device=DEVICE)\n","    batch_negative_embeddings = torch.full((len(batch_query_index_list), max(batch_total_negatives_list), 768), float('nan'), device=DEVICE)\n","    for i, (query_index, total_positives, total_negatives) in enumerate(zip(batch_query_index_list, batch_total_positives_list, batch_total_negatives_list)):\n","        positive_index_list = get_positive_indices(dataset, query_index, baseline_retrieved_passage_indices[query_index, :], total_positives, mode)\n","        negative_index_list = get_negative_indices(dataset, query_index, baseline_retrieved_passage_indices[query_index, :], total_negatives, mode)\n","        batch_positive_embeddings[i, :len(positive_index_list), :] = torch.from_numpy(passage_embeddings[positive_index_list, :]).to(device=DEVICE)\n","        batch_negative_embeddings[i, :len(negative_index_list), :] = torch.from_numpy(passage_embeddings[negative_index_list, :]).to(device=DEVICE)\n","    return batch_positive_embeddings, batch_negative_embeddings\n","\n","def get_loss(batch_mapped_query_embeddings:torch.Tensor, batch_positive_embeddings:torch.Tensor, batch_negative_embeddings:torch.Tensor, margin:float, norm_order:int) -> torch.Tensor:\n","    batch_aggregated_positive_embeddings = torch.nanmean(batch_positive_embeddings, dim=1)\n","    batch_aggregated_negative_embeddings = torch.nanmean(batch_negative_embeddings, dim=1)\n","    batch_positive_scores = torch.norm(batch_mapped_query_embeddings - batch_aggregated_positive_embeddings, p=norm_order, dim=1)\n","    batch_negative_scores = torch.norm(batch_mapped_query_embeddings - batch_aggregated_negative_embeddings, p=norm_order, dim=1)\n","    positive_loss = torch.nanmean(torch.abs(batch_positive_scores)**2)\n","    negative_loss = torch.nanmean(torch.relu(margin - batch_negative_scores)**2)\n","    loss = None\n","    if not torch.isnan(positive_loss) and not torch.isnan(negative_loss):\n","        loss = positive_loss + negative_loss\n","    elif not torch.isnan(positive_loss):\n","        loss = positive_loss\n","    elif not torch.isnan(negative_loss):\n","        loss = negative_loss\n","    return loss\n","\n","def get_llm_response(query:str, passage_list:list[str]):\n","    groq_client = Groq(api_key=GROQ_API_KEY)\n","    chat_completion = groq_client.chat.completions.create(\n","        messages=[\n","            {'role': 'system', 'content': (\n","                'You are an AI assistant tasked to work in a Retrieval Augmented Generation architecture.'\n","                '\\nYou recieve a question and a list of documents.'\n","                '\\nYou answer the question only based on the provided context.'\n","                '\\nNever use your general knowledge in your response.'\n","            )},\n","            {'role': 'user', 'content': (\n","                'The following is the question:'\n","                '\\n' + query + ''\n","                'And this is the document list:'\n","                '\\n' + '\\n'.join(passage_list) + ''\n","            )}\n","        ],\n","        model=GROQ_MODEL,\n","        max_tokens=512\n","    )\n","    return chat_completion.choices[0].message.content"],"metadata":{"id":"58apNqV5ispT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Development"],"metadata":{"id":"pc5Lh8iPQnK7"}},{"cell_type":"code","source":["total_epochs = 50\n","patience = 3\n","\n","batch_size = 512\n","learning_rate = 0.0006598\n","preferred_total = 2\n","positive_tendency = 0.75\n","mode = 'worst-worst'\n","margin = 0.2068\n","norm_order = 3\n","\n","mapper = Mapper().to(device=DEVICE)\n","optimizer = torch.optim.Adam(mapper.parameters(), lr=learning_rate)\n","\n","best_mapper = mapper\n","best_validation_avg_pessimistic_mrr = -float('inf')\n","total_epochs_since_improvement = 0\n","for epoch in range(total_epochs):\n","\n","    with tqdm(total=len(ms_marco_dataset.train_set) // batch_size, desc=f'Epoch {epoch + 1:02}/{total_epochs}') as pbar:\n","\n","        mapper.train()\n","        loss_list = list[float]()\n","        for step in range(len(ms_marco_dataset.train_set) // batch_size):\n","\n","            batch_query_index_list = random.sample(list(ms_marco_dataset.train_set), batch_size)\n","            batch_query_embeddings = torch.from_numpy(ms_marco_b3_query_embeddings[batch_query_index_list, :]).to(device=DEVICE)\n","            batch_mapped_query_embeddings = mapper(batch_query_embeddings)\n","\n","            batch_positive_embeddings, batch_negative_embeddings = get_targets(ms_marco_dataset, ms_marco_b3_passage_embeddings, ms_marco_b3_retrieved_passage_indices, batch_query_index_list, preferred_total, positive_tendency, mode)\n","            loss = get_loss(batch_mapped_query_embeddings, batch_positive_embeddings, batch_negative_embeddings, margin, norm_order)\n","            if loss is not None:\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","                loss_list.append(loss.item())\n","\n","            pbar.set_postfix_str(f'loss: {np.mean(loss_list):.4f}', refresh=False)\n","            pbar.update(1)\n","\n","        avg_loss = np.mean(loss_list)\n","\n","        mapper.eval()\n","        with torch.no_grad():\n","\n","            sample_train_query_index_list = random.sample(list(ms_marco_dataset.train_set), batch_size)\n","            sample_train_query_embeddings = torch.from_numpy(ms_marco_b3_query_embeddings[sample_train_query_index_list, :]).to(device=DEVICE)\n","            sample_train_mapped_query_embeddings = mapper(sample_train_query_embeddings)\n","            sample_train_retrieved_passage_indices = ms_marco_semantic_searcher.search(sample_train_mapped_query_embeddings.cpu().numpy())\n","            _, _, avg_train_optimistic_mrr, avg_train_pessimistic_mrr = ms_marco_dataset.get_metrics(sample_train_query_index_list, sample_train_retrieved_passage_indices)\n","\n","            validation_query_index_list = list(ms_marco_dataset.validation_set)\n","            validation_query_embeddings = torch.from_numpy(ms_marco_b3_query_embeddings[validation_query_index_list, :]).to(device=DEVICE)\n","            validation_mapped_query_embeddings = mapper(validation_query_embeddings)\n","            validation_retrieved_passage_indices = ms_marco_semantic_searcher.search(validation_mapped_query_embeddings.cpu().numpy())\n","            _, _, avg_validation_optimistic_mrr, avg_validation_pessimistic_mrr = ms_marco_dataset.get_metrics(validation_query_index_list, validation_retrieved_passage_indices)\n","\n","            pbar.set_postfix_str(f'train (loss: {avg_loss:.4f}, o-mrr: {avg_train_optimistic_mrr:.4f}, p-mrr: {avg_train_pessimistic_mrr:.4f}), validation (o-mrr: {avg_validation_optimistic_mrr:.4f}, p-mrr: {avg_validation_pessimistic_mrr:.4f})', refresh=True)\n","\n","    if avg_validation_pessimistic_mrr > best_validation_avg_pessimistic_mrr:\n","        best_mapper = copy.deepcopy(mapper)\n","        best_validation_avg_pessimistic_mrr = avg_validation_pessimistic_mrr\n","        total_epochs_since_improvement = 0\n","    else:\n","        total_epochs_since_improvement += 1\n","    if total_epochs_since_improvement >= patience:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lfBF4CMeKN76","executionInfo":{"status":"ok","timestamp":1740431557454,"user_tz":-60,"elapsed":5126,"user":{"displayName":"Homayoun Afshari","userId":"15693660695795650579"}},"outputId":"a1e84a37-5c57-41b6-b367-57cd915967dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 01/50: 100%|██████████| 1/1 [00:00<00:00,  3.90it/s, train (loss: 0.1115, o-mrr: 0.9898, p-mrr: 0.7792), validation (o-mrr: 1.0000, p-mrr: 0.7972)]\n","Epoch 02/50: 100%|██████████| 1/1 [00:00<00:00,  3.93it/s, train (loss: 0.1068, o-mrr: 0.9852, p-mrr: 0.7902), validation (o-mrr: 1.0000, p-mrr: 0.8029)]\n","Epoch 03/50: 100%|██████████| 1/1 [00:00<00:00,  3.84it/s, train (loss: 0.1025, o-mrr: 0.9861, p-mrr: 0.8226), validation (o-mrr: 1.0000, p-mrr: 0.8061)]\n","Epoch 04/50: 100%|██████████| 1/1 [00:00<00:00,  4.07it/s, train (loss: 0.0993, o-mrr: 0.9864, p-mrr: 0.8069), validation (o-mrr: 1.0000, p-mrr: 0.8093)]\n","Epoch 05/50: 100%|██████████| 1/1 [00:00<00:00,  4.17it/s, train (loss: 0.0950, o-mrr: 0.9864, p-mrr: 0.8221), validation (o-mrr: 1.0000, p-mrr: 0.8105)]\n","Epoch 06/50: 100%|██████████| 1/1 [00:00<00:00,  4.04it/s, train (loss: 0.0919, o-mrr: 0.9867, p-mrr: 0.8121), validation (o-mrr: 1.0000, p-mrr: 0.8106)]\n","Epoch 07/50: 100%|██████████| 1/1 [00:00<00:00,  3.83it/s, train (loss: 0.0897, o-mrr: 0.9839, p-mrr: 0.8324), validation (o-mrr: 1.0000, p-mrr: 0.8098)]\n","Epoch 08/50: 100%|██████████| 1/1 [00:00<00:00,  4.08it/s, train (loss: 0.0873, o-mrr: 0.9847, p-mrr: 0.8263), validation (o-mrr: 1.0000, p-mrr: 0.8117)]\n","Epoch 09/50: 100%|██████████| 1/1 [00:00<00:00,  4.17it/s, train (loss: 0.0849, o-mrr: 0.9871, p-mrr: 0.8433), validation (o-mrr: 1.0000, p-mrr: 0.8117)]\n","Epoch 10/50: 100%|██████████| 1/1 [00:00<00:00,  4.09it/s, train (loss: 0.0830, o-mrr: 0.9887, p-mrr: 0.8368), validation (o-mrr: 1.0000, p-mrr: 0.8125)]\n","Epoch 11/50: 100%|██████████| 1/1 [00:00<00:00,  3.82it/s, train (loss: 0.0809, o-mrr: 0.9887, p-mrr: 0.8438), validation (o-mrr: 1.0000, p-mrr: 0.8122)]\n","Epoch 12/50: 100%|██████████| 1/1 [00:00<00:00,  3.60it/s, train (loss: 0.0790, o-mrr: 0.9911, p-mrr: 0.8701), validation (o-mrr: 1.0000, p-mrr: 0.8118)]\n","Epoch 13/50: 100%|██████████| 1/1 [00:00<00:00,  4.16it/s, train (loss: 0.0786, o-mrr: 0.9864, p-mrr: 0.8610), validation (o-mrr: 1.0000, p-mrr: 0.8132)]\n","Epoch 14/50: 100%|██████████| 1/1 [00:00<00:00,  3.81it/s, train (loss: 0.0763, o-mrr: 0.9862, p-mrr: 0.8590), validation (o-mrr: 1.0000, p-mrr: 0.8122)]\n","Epoch 15/50: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s, train (loss: 0.0746, o-mrr: 0.9888, p-mrr: 0.8607), validation (o-mrr: 1.0000, p-mrr: 0.8133)]\n","Epoch 16/50: 100%|██████████| 1/1 [00:00<00:00,  2.74it/s, train (loss: 0.0723, o-mrr: 0.9874, p-mrr: 0.8604), validation (o-mrr: 1.0000, p-mrr: 0.8132)]\n","Epoch 17/50: 100%|██████████| 1/1 [00:00<00:00,  2.31it/s, train (loss: 0.0726, o-mrr: 0.9896, p-mrr: 0.8785), validation (o-mrr: 1.0000, p-mrr: 0.8121)]\n","Epoch 18/50: 100%|██████████| 1/1 [00:00<00:00,  2.45it/s, train (loss: 0.0705, o-mrr: 0.9888, p-mrr: 0.8618), validation (o-mrr: 1.0000, p-mrr: 0.8130)]\n"]}]},{"cell_type":"markdown","source":["## Server"],"metadata":{"id":"a7K6M2dKsrSX"}},{"cell_type":"code","source":["ngrok.set_auth_token(NGROK_AUTHENTICATION_TOKEN)\n","public_url = ngrok.connect(INTERNAL_PORT).public_url\n","print(f'Public URL: {public_url}')\n","\n","app = fastapi.FastAPI()\n","app.add_middleware(\n","    CORSMiddleware,\n","    allow_origins=['*'],\n","    allow_credentials=True,\n","    allow_methods=['*'],\n","    allow_headers=['*'],\n",")\n","\n","@app.get('/')\n","async def serve_index() -> fastapi.responses.HTMLResponse:\n","    with open('index.html', 'r') as html_file:\n","        html_content = html_file.read()\n","    html_content = html_content.replace('<PUBLIC_URL>', public_url)\n","    return fastapi.responses.HTMLResponse(content=html_content)\n","\n","@app.get('/{filename}')\n","async def serve_static(filename: str) -> fastapi.responses.FileResponse:\n","    if not os.path.exists(filename):\n","        return fastapi.responses.JSONResponse(content={'error': 'File not found'}, status_code=404)\n","    return fastapi.responses.FileResponse(filename)\n","\n","@app.post('/request')\n","async def receive_data(request: fastapi.Request) -> fastapi.responses.JSONResponse:\n","    data:Any = await request.json()\n","    if data == 'SEND_QUERIES':\n","        random_query_index_list = random.sample(range(len(ms_marco_dataset.query_list)), 10)\n","        result = {query_index: ms_marco_dataset.query_list[query_index] for query_index in random_query_index_list}\n","    else:\n","        chosen_query_index_list = [int(data)]\n","        chosen_query = ms_marco_dataset.query_list[chosen_query_index_list[0]]\n","        chosen_query_embedding = torch.from_numpy(ms_marco_b3_query_embeddings[chosen_query_index_list, :]).to(device=DEVICE)\n","        mapper.eval()\n","        with torch.no_grad():\n","            chosen_query_mapped_query_embedding = mapper(chosen_query_embedding)\n","        correct_passage_index_list = ms_marco_semantic_searcher.search(chosen_query_mapped_query_embedding.cpu().numpy())[0, :5].tolist()\n","        correct_passage_list = [ms_marco_dataset.passage_list[passage_index] for passage_index in correct_passage_index_list]\n","        correct_answer = get_llm_response(chosen_query, correct_passage_list)\n","        wrong_passage_index_list = random.sample(range(len(ms_marco_dataset.passage_list)), random.randint(3, 7))\n","        wrong_passage_list = [ms_marco_dataset.passage_list[passage_index] for passage_index in wrong_passage_index_list]\n","        wrong_answer = get_llm_response(chosen_query, wrong_passage_list)\n","        result = {\n","            'good': {'docs': correct_passage_list, 'answer': correct_answer},\n","            'bad': {'docs': wrong_passage_list, 'answer': wrong_answer}\n","        }\n","    return fastapi.responses.JSONResponse(content=result)\n","\n","nest_asyncio.apply()\n","uvicorn.run(app, host='0.0.0.0', port=INTERNAL_PORT)"],"metadata":{"id":"SFjNKu-u_7M8"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["XFd-W_zwTIUV","dUZQ4D2K79Bb","sHyt5bXp8AYv","B9ja7XCi8HUx","SDuRJDRl8Rym","I6-WSuvnegif","fEy-ALy0P8r6","pc5Lh8iPQnK7"],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}